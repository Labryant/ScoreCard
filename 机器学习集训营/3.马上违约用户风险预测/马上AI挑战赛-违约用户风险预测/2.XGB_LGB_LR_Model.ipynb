{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "'''单变量特征选取'''\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "'''去除方差小的特征'''\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "'''循环特征选取'''\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "'''RFE_CV'''\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''训练集'''\n",
    "train_auth_info = pd.read_csv('./data/AI_risk_train_V3.0/train_auth_info.csv', low_memory=False)\n",
    "train_bankcard_info = pd.read_csv('./data/AI_risk_train_V3.0/train_bankcard_info.csv', low_memory=False)\n",
    "train_credit_info = pd.read_csv('./data/AI_risk_train_V3.0/train_credit_info.csv', low_memory=False)\n",
    "train_order_info = pd.read_csv('./data/AI_risk_train_V3.0/train_order_info.csv', low_memory=False)\n",
    "train_recieve_addr_info = pd.read_csv('./data/AI_risk_train_V3.0/train_recieve_addr_info.csv', low_memory=False)\n",
    "train_user_info = pd.read_csv('./data/AI_risk_train_V3.0/train_user_info.csv', low_memory=False)\n",
    "train_target = pd.read_csv('./data/AI_risk_train_V3.0/train_target.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''测试集'''\n",
    "test_auth_info = pd.read_csv('./data/AI_Risk_BtestData_V1.0/Btest_auth_info.csv', low_memory=False)\n",
    "test_bankcard_info = pd.read_csv('./data/AI_Risk_BtestData_V1.0/Btest_bankcard_info.csv', low_memory=False)\n",
    "test_credit_info = pd.read_csv('./data/AI_Risk_BtestData_V1.0/Btest_credit_info.csv', low_memory=False)\n",
    "test_order_info = pd.read_csv('./data/AI_Risk_BtestData_V1.0/Btest_order_info.csv', low_memory=False)\n",
    "test_recieve_addr_info = pd.read_csv('./data/AI_Risk_BtestData_V1.0/Btest_recieve_addr_info.csv', low_memory=False)\n",
    "test_user_info = pd.read_csv('./data/AI_Risk_BtestData_V1.0/Btest_user_info.csv', low_memory=False)\n",
    "test_list = pd.read_csv('./data/AI_Risk_BtestData_V1.0/Btest_list.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_set(group):\n",
    "    return set(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_auc(list_one, list_two):\n",
    "    '''计算AUC值'''\n",
    "    positive = []\n",
    "    negative = []\n",
    "    for index in range(len(list_one)):\n",
    "        if list_one[index] == 1:\n",
    "            positive.append(index)\n",
    "        else:\n",
    "            negative.append(index)\n",
    "    SUM = 0\n",
    "    for i in positive:\n",
    "        for j in negative:\n",
    "            if list_two[i] > list_two[j]:\n",
    "                SUM += 1\n",
    "            elif list_two[i] == list_two[j]:\n",
    "                SUM += 0.5\n",
    "            else:\n",
    "                pass\n",
    "    return SUM / (len(positive)*len(negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "用户信用积分\n",
    "用户额度是否为 0\n",
    "用户已使用的额度\n",
    "用户的信用额度\n",
    "用户剩余的额度\n",
    "用户额度使用率\n",
    "用户额度排名\n",
    "是否所有信息都为空，除了 id\n",
    "是否所有信息都为 0，除了 id\n",
    "信用额度是否为 0\n",
    "信用积分是否为 0\n",
    "用户是否还有剩余的额度\n",
    "'''\n",
    "def extract_credit_info(credit_info):\n",
    "    '''提取credit_info表 特征'''\n",
    "    credit_info['credit_score'] = credit_info['credit_score'].fillna(credit_info['credit_score'].mean())\n",
    "    credit_info['quota_is_zero'] = [1 if i != 0.0 else 0 for i in credit_info.quota]  # 是否有信用额度 #\n",
    "    credit_info['overdraft'] = credit_info['overdraft'].fillna(0)\n",
    "    credit_info['quota'] = credit_info['quota'].fillna(0)\n",
    "    credit_info['quota_surplus'] = credit_info['quota'] - credit_info['overdraft']\n",
    "    # credit_info['quota_rate'] = (credit_info['overdraft'] / credit_info['quota']).fillna(0)\n",
    "    credit_info['quota_rate'] = credit_info[['overdraft', 'quota']].apply(lambda x: x.overdraft / x.quota if x.quota != 0 else 0, axis=1)\n",
    "    credit_info['credit_score_rank'] = credit_info['credit_score'].rank(method='first', ascending=False)\n",
    "\n",
    "    credit_info.loc[:, 'all_is_null'] = credit_info[['credit_score', 'overdraft', 'quota']].apply(lambda x: 1 if ((x.credit_score is not np.nan) and (x.overdraft is not np.nan) and (x.quota is not np.nan)) else 0, axis=1)\n",
    "    credit_info.loc[:, 'all_is_zero'] = credit_info[['credit_score', 'overdraft', 'quota']].apply(lambda x: 1 if ((x.credit_score == 0) and (x.overdraft == 0) and (x.quota == 0)) else 0, axis=1)\n",
    "    credit_info.loc[:, 'quota_is_zero'] = credit_info[['quota']].apply(lambda x: 1 if x.quota == 0 else 0, axis=1)\n",
    "    credit_info.loc[:, 'credit_score_is_null'] = credit_info[['credit_score']].apply(lambda x: 1 if x.credit_score == 0 else 0, axis=1)\n",
    "    credit_info.loc[:, 'quota_surplus_is_null'] = credit_info[['quota_surplus', 'quota']].apply(lambda x: 1 if (x.quota_surplus == 0) and (x.quota != 0) else 0, axis=1)\n",
    "\n",
    "    '''归一化'''\n",
    "    credit_info[['credit_score', 'overdraft', 'quota', 'quota_surplus', 'credit_score_rank']] = credit_info[['credit_score', 'overdraft', 'quota', 'quota_surplus', 'credit_score_rank']].apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "    return credit_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''年龄特征'''\n",
    "def is_valid_date(strdate):\n",
    "    '''''判断是否是一个有效的日期字符串'''\n",
    "    try:\n",
    "        if \":\" in strdate:\n",
    "            time.strptime(strdate, \"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            time.strptime(strdate, \"%Y-%m-%d\")\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "用户生日是否是“0000-00-00”\n",
    "用户性别的 one-hot 编码\n",
    "用户婚姻状况的 one-hot 编码\n",
    "用户会员等级的 one-hot 编码\n",
    "用户是否绑定 QQ\n",
    "用户是否绑定微信号\n",
    "用户学历是否是“硕士、其它、博士”\n",
    "用户身份证号是否为空\n",
    "用户会员收入的 one-hot 编码\n",
    "'''\n",
    "def extract_user_info(user_info):\n",
    "    '''提取 user_info表 特征'''\n",
    "    feature = user_info[['id']]\n",
    "    feature.loc[:, 'birthday_is_zero'] = user_info[['birthday']].apply(lambda x: 1 if x.birthday == '0000-00-00' else 0, axis=1)\n",
    "    feature.loc[:, 'sex_not_male'] = user_info[['sex']].apply(lambda x: 1 if x.sex != '女' else 0, axis=1)\n",
    "    feature.loc[:, 'female'] = user_info[['sex']].apply(lambda x: 1 if x.sex == '男' else 0, axis=1)\n",
    "    feature.loc[:, 'male'] = user_info[['sex']].apply(lambda x: 1 if x.sex == '女' else 0, axis=1)\n",
    "    feature.loc[:, 'sex_secret'] = user_info[['sex']].apply(lambda x: 1 if x.sex == '保密' else 0, axis=1)    # 0.69504936432\n",
    "    ##\n",
    "    feature.loc[:, 'merriage1'] = user_info[['merriage']].apply(lambda x: 1 if x.merriage == '未婚' else 0, axis=1)\n",
    "    feature.loc[:, 'merriage2'] = user_info[['merriage']].apply(lambda x: 1 if x.merriage == '已婚' else 0, axis=1)\n",
    "    feature.loc[:, 'merriage3'] = user_info[['merriage']].apply(lambda x: 1 if x.merriage == '保密' else 0, axis=1)\n",
    "    feature.loc[:, 'merriage_is_null'] = user_info[['merriage']].apply(lambda x: 1 if x.merriage is np.nan else 0, axis=1)   # 0.700624700466\n",
    "    ####\n",
    "    feature.loc[:, 'account_grade1'] = user_info[['account_grade']].apply(lambda x: 1 if x.account_grade == '注册会员' else 0, axis=1)\n",
    "    feature.loc[:, 'account_grade2'] = user_info[['account_grade']].apply(lambda x: 1 if x.account_grade == '铜牌会员' else 0, axis=1)\n",
    "    feature.loc[:, 'account_grade3'] = user_info[['account_grade']].apply(lambda x: 1 if x.account_grade == '银牌会员' else 0, axis=1)\n",
    "    feature.loc[:, 'account_grade4'] = user_info[['account_grade']].apply(lambda x: 1 if x.account_grade == '金牌会员' else 0, axis=1)\n",
    "    feature.loc[:, 'account_grade5'] = user_info[['account_grade']].apply(lambda x: 1 if x.account_grade == '钻石会员' else 0, axis=1)\n",
    "    feature.loc[:, 'account_grade_is_null'] = user_info[['account_grade']].apply(lambda x: 1 if x.account_grade is np.nan else 0, axis=1)\n",
    "    ###\n",
    "    feature.loc[:, 'qq_bound_is_null'] = user_info[['qq_bound']].apply(lambda x: 1 if x.qq_bound is np.nan else 0, axis=1)\n",
    "    feature.loc[:, 'wechat_bound_is_null'] = user_info[['wechat_bound']].apply(lambda x: 1 if x.wechat_bound is np.nan else 0, axis=1)\n",
    "    feature.loc[:, 'degree'] = user_info[['degree']].apply(lambda x: 1 if (x.degree == '硕士') | (x.degree == '其他') | (x.degree == '博士') else 0, axis=1)\n",
    "    feature.loc[:, 'id_card_is_null'] = user_info[['id_card']].apply(lambda x: 1 if x.id_card is np.nan else 0, axis=1)\n",
    "    #####\n",
    "    feature.loc[:, 'income1'] = [1 if index == '4000-5999元' else 0 for index in user_info['income']]\n",
    "    feature.loc[:, 'income2'] = [1 if index == '8000元以上' else 0 for index in user_info['income']]\n",
    "    feature.loc[:, 'income3'] = [1 if index == '2000-3999元' else 0 for index in user_info['income']]\n",
    "    feature.loc[:, 'income4'] = [1 if index == '6000-7999元' else 0 for index in user_info['income']]\n",
    "    feature.loc[:, 'income5'] = [1 if index == '2000元以下' else 0 for index in user_info['income']]     # 0.775891365882 #\n",
    "\n",
    "    ####\n",
    "    user_info['birthday_two'] = user_info[['birthday']].apply(lambda index: is_valid_date(index.birthday), axis=1)\n",
    "    user_info['birthday'] = user_info[['birthday']].apply(lambda index: 0 if (index.birthday is np.nan) or (index.birthday == '0000-00-00') else index.birthday[0:4], axis=1)\n",
    "    user_info['age'] = user_info[['birthday', 'birthday_two']].apply(lambda x: 2018 - int(x.birthday) if x.birthday_two is True else 0, axis=1)\n",
    "    # print(user_info[['birthday_two', 'age']])\n",
    "    feature.loc[:, 'age_one'] = user_info[['age']].apply(lambda x: 1 if x.age <= 18 and x.age > 0 else 0, axis=1)\n",
    "    feature.loc[:, 'age_two'] = user_info[['age']].apply(lambda x: 1 if x.age <= 30 and x.age > 18 else 0, axis=1)\n",
    "    feature.loc[:, 'age_three'] = user_info[['age']].apply(lambda x: 1 if x.age <= 60 and x.age > 30 else 0, axis=1)\n",
    "    feature.loc[:, 'age_four'] = user_info[['age']].apply(lambda x: 1 if x.age <= 100 and x.age > 60 else 0, axis=1)\n",
    "    feature.loc[:, 'age_five'] = user_info[['age']].apply(lambda x: 1 if x.age > 100 and x.age == 0 else 0, axis=1)\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "用户记录中是否有除 id 外都为空\n",
    "'addr_id', 'region', 'phone', 'fix_phone', 'receiver_md5'是否同时为空\n",
    "用户的记录数\n",
    "用户收获地址中的省份离散特征\n",
    "用户收获地址中有多少不同的省份\n",
    "'''\n",
    "def extract_recieve_addr_info(recieve_addr_info):\n",
    "    '''提取 recieve_addr_info表 特征'''\n",
    "    recieve_addr_info['all_null'] = recieve_addr_info[['addr_id', 'region', 'phone', 'fix_phone', 'receiver_md5']].apply(lambda x: 1 if (x.addr_id is np.nan) and (x.region is np.nan) and (x.phone is np.nan) and (x.fix_phone is np.nan) | (x.receiver_md5 is np.nan) else 0, axis=1)\n",
    "    feature = recieve_addr_info.drop_duplicates(['id'])[['id']]\n",
    "    recieve_addr_info['index'] = recieve_addr_info.index\n",
    "    all_is_null = pd.pivot_table(recieve_addr_info, index='id', values='all_null', aggfunc='min').reset_index()\n",
    "    addr_id = pd.pivot_table(recieve_addr_info, index='id', values='index', aggfunc='count').reset_index().rename(columns={'index': 'record_count'})\n",
    "    feature = feature.merge(all_is_null, on='id', how='left')\n",
    "    feature = feature.merge(addr_id, on='id', how='left')\n",
    "    province = {'甘肃', '云南', '贵州', '河南', '黑龙', '香港', '北京', '湖南', '江苏', '青海', '宁夏', '内蒙', '浙江', '吉林', '海南', '福建', '重庆', '台湾', '陕西', '湖北', '江西', '辽宁', '山西', '西藏', '广东', '安徽', '四川', '河北', '山东', '上海', '广西', '新疆', '天津', 'null'}\n",
    "\n",
    "    train_recieve_addr_info['province'] = train_recieve_addr_info[['region']].apply(lambda x: 'null' if x.region is np.nan else x.region[0:2], axis=1)\n",
    "    city_set = pd.pivot_table(train_recieve_addr_info, index='id', values='province', aggfunc=return_set).reset_index()\n",
    "    for string in list(province):\n",
    "        city_set[string] = [1 if string in index else 0 for index in city_set['province']]\n",
    "    city_set['province'] = city_set[['province']].apply(lambda x: x.province.clear() if 'null' in x.province else x.province, axis=1)\n",
    "    city_set['province_len'] = [0 if index is None else len(index) for index in city_set['province']]\n",
    "\n",
    "    feature = feature.merge(city_set.drop(['province'], axis=1), on='id', how='left')\n",
    "    # print(feature)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "用户有多少条记录\n",
    "用户有多少个不同的手机号码\n",
    "用户的储蓄卡的数量\n",
    "用户的信用卡的数量\n",
    "用户是否有信用卡\n",
    "用户有几种不同类型的银行卡\n",
    "用户银行卡数量是否大于 6\n",
    "用户是否只有一张银行卡\n",
    "'''\n",
    "def extract_bankcard_info(bankcard_info):\n",
    "    ''' 提取 bankcard_info表 特征 '''\n",
    "\n",
    "    def cal_store_card_num(group):\n",
    "        flag = 0\n",
    "        for index in group:\n",
    "            if index == '储蓄卡':\n",
    "                flag += 1\n",
    "        return flag\n",
    "\n",
    "    def if_have_credit_card(group):\n",
    "        for index in group:\n",
    "            if index == '信用卡':\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        return 0\n",
    "\n",
    "    def list_set(group):\n",
    "        return len(set(group))\n",
    "\n",
    "    bankcard_info = bankcard_info.drop_duplicates()\n",
    "    feature = bankcard_info.drop_duplicates(['id'])[['id']]\n",
    "    card_record_count = pd.pivot_table(bankcard_info, index='id', values='phone', aggfunc='count').reset_index().rename(columns={'phone': 'card_record_count'})\n",
    "    phone_count = pd.pivot_table(bankcard_info, index='id', values='phone', aggfunc=list_set).reset_index().rename(columns={'phone': 'phone_count'})\n",
    "    store_card_count = pd.pivot_table(bankcard_info, index='id', values='card_type', aggfunc=cal_store_card_num).reset_index().rename(columns={'card_type': 'store_card_count'})\n",
    "    have_credit_card = pd.pivot_table(bankcard_info, index='id', values='card_type', aggfunc=if_have_credit_card).reset_index().rename(columns={'card_type': 'have_credit_card'})\n",
    "    card_category_count = pd.pivot_table(bankcard_info, index='id', values='card_type', aggfunc=list_set).reset_index().rename(columns={'card_type': 'card_category_count'})\n",
    "\n",
    "    feature = feature.merge(phone_count, on='id', how='left')\n",
    "    feature = feature.merge(card_record_count, on='id', how='left')\n",
    "    feature = feature.merge(store_card_count, on='id', how='left')\n",
    "    feature = feature.merge(have_credit_card, on='id', how='left')\n",
    "    feature = feature.merge(card_category_count, on='id', how='left')\n",
    "    feature['credit_count'] = feature['card_record_count'] - feature['store_card_count']\n",
    "    feature['card_count_one'] = feature[['card_record_count']].apply(lambda x: 1 if x.card_record_count > 6 else 0, axis=1)\n",
    "    feature['record_is_unique'] = feature[['card_record_count']].apply(lambda x: 1 if x.card_record_count == 1 else 0, axis=1)\n",
    "    # print(feature)\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "身份证账号是否为空\n",
    "认证时间是否为空\n",
    "电话号码是否为空\n",
    "是否所有信息都为空，除了 id\n",
    "是否所有信息都不为空\n",
    "认证时间和身份证是否同时为空\n",
    "认证时间和手机号码是否同时为空\n",
    "'''\n",
    "def extract_auth_info(auth_info):\n",
    "    '''提取 auth_info表 特征'''\n",
    "    feature = auth_info[['id']]\n",
    "    feature.loc[:, 'auth_id_card_is_null'] = auth_info[['id_card']].apply(lambda x: 1 if x.id_card is not np.nan else 0, axis=1)\n",
    "    feature.loc[:, 'auth_time_is_null'] = auth_info[['auth_time']].apply(lambda x: 1 if x.auth_time is not np.nan else 0, axis=1)\n",
    "    feature.loc[:, 'phone_is_null'] = auth_info[['phone']].apply(lambda x: 1 if x.phone is not np.nan else 0, axis=1)\n",
    "    feature.loc[:, 'all_is_null'] = auth_info[['id_card', 'auth_time', 'phone']].apply(lambda x: 1 if ((x.id_card is np.nan) and (x.auth_time is np.nan) and (x.phone is np.nan)) else 0, axis=1)\n",
    "    feature.loc[:, 'all_not_null'] = auth_info[['id_card', 'auth_time', 'phone']].apply(lambda x: 1 if ((x.id_card is not np.nan) and (x.auth_time is not np.nan) and (x.phone is not np.nan)) else 0, axis=1)\n",
    "    feature.loc[:, 'card_time_is_null'] = auth_info[['id_card', 'auth_time']].apply(lambda x: 1 if ((x.id_card is np.nan) and (x.auth_time is np.nan)) else 0, axis=1)\n",
    "    feature.loc[:, 'time_phone_is_null'] = auth_info[['auth_time', 'phone']].apply(lambda x: 1 if ((x.phone is np.nan) and (x.auth_time is np.nan)) else 0, axis=1)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "用户记录是否有除 id 外都为空\n",
    "用户关于商品单价的统计特征\n",
    "用户关于订单金额的统计特征\n",
    "支付方式的离散特征\n",
    "订单状态的离散特征\n",
    "'''\n",
    "def extract_order_info(order_info):\n",
    "    '''提取 order_info表 特征'''\n",
    "    def cal_set(group):\n",
    "        return len(set(group))\n",
    "\n",
    "    '''求标准差'''\n",
    "    def cal_std(group):\n",
    "        return np.std(group)\n",
    "\n",
    "    feature = order_info.drop_duplicates(['id'])[['id']]\n",
    "    # amt_order, type_pay, time_order, sts_order, phone, unit_price, no_order_md5, name_rec_md5, product_id_md5\n",
    "    order_info['order_all_is_null'] = order_info.apply(lambda x: 1 if ((x.amt_order is np.nan) and (x.type_pay is np.nan) and (x.time_order is np.nan) and (x.sts_order is np.nan)) else 0, axis=1)\n",
    "    order_all_is_null = pd.pivot_table(order_info[['id', 'order_all_is_null']], index='id', values='order_all_is_null', aggfunc='max').reset_index()\n",
    "\n",
    "    '''均值填充amt_order属性'''\n",
    "    order_info_amt = order_info[['amt_order']]\n",
    "    order_info_amt = order_info_amt[order_info_amt['amt_order'].notnull()]\n",
    "    order_info_amt = order_info_amt[order_info_amt['amt_order'] != 'null']\n",
    "    order_info_amt['amt_order'] = [float(index) for index in order_info_amt['amt_order']]\n",
    "    mean = order_info_amt['amt_order'].mean()\n",
    "    order_info['amt_order'] = order_info['amt_order'].fillna(mean)\n",
    "    order_info['amt_order'] = [mean if index == 'null' else index for index in order_info['amt_order']]\n",
    "    order_info['amt_order'] = [float(index) for index in order_info['amt_order']]\n",
    "\n",
    "    order_info['unit_price'] = order_info[['amt_order', 'unit_price']].apply(lambda x: x.amt_order if np.isnan(x.unit_price) else x.unit_price, axis=1)\n",
    "    unit_price_mean = pd.pivot_table(order_info[['id', 'unit_price']], index='id', values='unit_price', aggfunc='mean').reset_index().rename(columns={'unit_price': 'unit_price_mean'})\n",
    "    unit_price_max = pd.pivot_table(order_info[['id', 'unit_price']], index='id', values='unit_price', aggfunc='max').reset_index().rename(columns={'unit_price': 'unit_price_max'})\n",
    "    unit_price_min = pd.pivot_table(order_info[['id', 'unit_price']], index='id', values='unit_price', aggfunc='min').reset_index().rename(columns={'unit_price': 'unit_price_min'})\n",
    "    unit_price_std = pd.pivot_table(order_info[['id', 'unit_price']], index='id', values='unit_price', aggfunc=cal_std).reset_index().rename(columns={'unit_price': 'unit_price_std'})\n",
    "\n",
    "    amt_order_mean = pd.pivot_table(order_info[['id', 'amt_order']], index='id', values='amt_order', aggfunc='mean').reset_index().rename(columns={'amt_order': 'amt_order_mean'})\n",
    "    amt_order_max = pd.pivot_table(order_info[['id', 'amt_order']], index='id', values='amt_order', aggfunc='max').reset_index().rename(columns={'amt_order': 'amt_order_max'})\n",
    "    amt_order_min = pd.pivot_table(order_info[['id', 'amt_order']], index='id', values='amt_order', aggfunc='min').reset_index().rename(columns={'amt_order': 'amt_order_min'})\n",
    "    amt_order_std = pd.pivot_table(order_info[['id', 'amt_order']], index='id', values='amt_order', aggfunc=cal_std).reset_index().rename(columns={'amt_order': 'amt_order_std'})\n",
    "    type_pay_count = pd.pivot_table(order_info[['id', 'type_pay']], index='id', values='type_pay', aggfunc=cal_set).reset_index().rename(columns={'type_pay': 'type_pay_count'})\n",
    "    sts_order_count = pd.pivot_table(order_info[['id', 'sts_order']], index='id', values='sts_order', aggfunc=cal_set).reset_index().rename(columns={'sts_order': 'sts_order_count'})\n",
    "    order_phone_count = pd.pivot_table(order_info[['id', 'phone']], index='id', values='phone', aggfunc=cal_set).reset_index().rename(columns={'phone': 'order_phone_count'})\n",
    "    name_rec_md5_count = pd.pivot_table(order_info[['id', 'name_rec_md5']], index='id', values='name_rec_md5', aggfunc=cal_set).reset_index().rename(columns={'name_rec_md5': 'name_rec_md5_count'})\n",
    "\n",
    "    feature = feature.merge(unit_price_mean, on='id', how='left')\n",
    "    feature = feature.merge(unit_price_max, on='id', how='left')\n",
    "    feature = feature.merge(unit_price_min, on='id', how='left')\n",
    "    feature = feature.merge(unit_price_std, on='id', how='left')\n",
    "\n",
    "    feature = feature.merge(order_all_is_null, on='id', how='left')\n",
    "    feature = feature.merge(amt_order_mean, on='id', how='left')\n",
    "    feature = feature.merge(amt_order_max, on='id', how='left')\n",
    "    feature = feature.merge(amt_order_min, on='id', how='left')\n",
    "    feature = feature.merge(amt_order_std, on='id', how='left')\n",
    "    feature = feature.merge(type_pay_count, on='id', how='left')\n",
    "    feature = feature.merge(sts_order_count, on='id', how='left')\n",
    "    feature = feature.merge(order_phone_count, on='id', how='left')\n",
    "    feature = feature.merge(name_rec_md5_count, on='id', how='left')\n",
    "    '''归一化'''\n",
    "    feature.iloc[:, 1:] = feature.iloc[:, 1:].apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))   # 0.791859501859 #\n",
    "    '''离散化特征'''\n",
    "    order_info['type_pay'] = order_info[['type_pay']].apply(lambda x: 'null' if x.type_pay is np.nan else x.type_pay, axis=1)\n",
    "    type_pay = pd.pivot_table(order_info, index='id', values='type_pay', aggfunc=return_set).reset_index()\n",
    "    \n",
    "    type_pay_category = {'定向京券支付', '白条支付', '在线+余额+限品东券', '高校代理-代理支付', '京券全额支付', '分期付款', '积分支付', '在线+限品东券', '定向东券', '东券混合支付', '余额', '京豆东券混合支付', '前台自付', '在线', '在线+东券支付', '上门自提', '公司转账', '在线支付', '在线支付 ', '在线+京豆', '邮局汇款', '在线+全品京券', '货到付款', '分期付款(招行)', '在线+全品东券', '余额+限品东券', '在线+京券支付', '在线+余额', '限品京券', 'null', '京豆支付', '在线预付', '定向京券', '混合支付', '全品京券', '京豆', '在线+定向东券', '京豆混合支付', '在线+限品京券', '高校代理-自己支付', '京券混合支付', '在线+东券'}\n",
    "    for string in list(type_pay_category):\n",
    "        type_pay[string] = [1 if string in index else 0 for index in type_pay['type_pay']]\n",
    "\n",
    "    type_pay['type_pay'] = type_pay[['type_pay']].apply(lambda x: x.type_pay.clear() if 'null' in x.type_pay else x.type_pay, axis=1)\n",
    "    type_pay['type_pay_len'] = [0 if index is None else len(index) for index in type_pay['type_pay']]\n",
    "    feature = feature.merge(type_pay.drop(['type_pay'], axis=1), on='id', how='left')\n",
    "\n",
    "    '''sts_order离散化'''\n",
    "    order_info['sts_order'] = order_info[['sts_order']].apply(lambda x: 'null' if x.sts_order is np.nan else x.sts_order, axis=1)\n",
    "    # sts_order_category = set(train_order_info['sts_order'])\n",
    "    sts_order = pd.pivot_table(order_info, index='id', values='sts_order', aggfunc=return_set).reset_index()\n",
    "    sts_order_category = {'null', '等待审核', '等待处理', '已退款', '已收货', '购买成功', '付款成功', '失败退款', '已完成', '预订结束', '退款完成', '正在出库', '订单已取消', '充值成功', '商品出库', '下单失败', '请上门自提', '已晒单', '充值失败;退款成功',\n",
    "                          '退款成功', '未入住', '等待收货', '配送退货', '出票失败', '等待付款确认', '缴费成功', '预约完成', '未抢中', '完成', '已取消', '出票成功', '抢票已取消', '等待付款', '已取消订单', '正在处理', '等待退款', '充值失败', '订单取消'}\n",
    "\n",
    "    for string in list(sts_order_category):\n",
    "        sts_order[string] = [1 if string in index else 0 for index in sts_order['sts_order']]\n",
    "\n",
    "    sts_order['sts_order'] = sts_order[['sts_order']].apply(lambda x: x.sts_order.clear() if 'null' in x.sts_order else x.sts_order, axis=1)\n",
    "    sts_order['sts_order_len'] = [0 if index is None else len(index) for index in sts_order['sts_order']]\n",
    "    # print(sts_order)\n",
    "    feature = feature.merge(sts_order.drop(['sts_order'], axis=1), on='id', how='left')\n",
    "\n",
    "    # print(feature)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "用户年龄\n",
    "用户注册天数\n",
    "用户借贷日期是否早于注册日期\n",
    "下订单时间与注册时间的天数差的最大、最小、平均\n",
    "'''\n",
    "def extract_time_feature(auth_info, target_list):\n",
    "    '''提取时间相关特征'''\n",
    "    feature = target_list[['id']]\n",
    "    target_list = target_list[['id', 'appl_sbm_tm']].merge(auth_info[['id', 'auth_time']], on='id', how='left')\n",
    "    target_list.loc[:, 'appl_sbm_tm'] = [index.split(' ')[0] for index in target_list['appl_sbm_tm']]\n",
    "    target_list['auth_time'] = target_list[['appl_sbm_tm', 'auth_time']].apply(lambda x: x.appl_sbm_tm if x.auth_time == '0000-00-00' else x.auth_time, axis=1)\n",
    "    target_list['auth_time'] = target_list[['appl_sbm_tm', 'auth_time']].apply(lambda x: x.appl_sbm_tm if x.auth_time is np.nan else x.auth_time, axis=1)\n",
    "    feature['feature_1'] = target_list[['appl_sbm_tm', 'auth_time']].apply(lambda x: 1 if x.appl_sbm_tm < x.auth_time else 0, axis=1)\n",
    "    feature['register_days'] = target_list[['appl_sbm_tm', 'auth_time']].apply(lambda x: (datetime(int(x.appl_sbm_tm.split('-')[0]), int(x.appl_sbm_tm.split('-')[1]), int(x.appl_sbm_tm.split('-')[2])) - datetime(int(x.auth_time.split('-')[0]), int(x.auth_time.split('-')[1]), int(x.auth_time.split('-')[2]))).days, axis=1)\n",
    "    # print(target_list)\n",
    "    # print(feature)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_order_payment_time(order_info, target_list):\n",
    "    str_len = len('2016-01-19 22:38:26')\n",
    "    feature = target_list[['id']]\n",
    "    target_list = target_list[['id', 'appl_sbm_tm']].merge(order_info[['id', 'time_order']], on='id', how='left')\n",
    "    target_list.loc[:, 'appl_sbm_tm'] = [index.split(' ')[0] for index in target_list['appl_sbm_tm']]\n",
    "    target_list['time_order'] = target_list[['appl_sbm_tm', 'time_order']].apply(lambda x: x.appl_sbm_tm if x.time_order is np.nan else x.time_order, axis=1)\n",
    "    target_list['time_order'] = target_list[['appl_sbm_tm', 'time_order']].apply(lambda x: x.appl_sbm_tm if len(x.time_order) != str_len else x.time_order, axis=1)\n",
    "    target_list.loc[:, 'time_order'] = [index.split(' ')[0] for index in target_list['time_order']]\n",
    "    target_list['days'] = target_list[['appl_sbm_tm', 'time_order']].apply(lambda x: (datetime(int(x.appl_sbm_tm.split('-')[0]), int(x.appl_sbm_tm.split('-')[1]), int(x.appl_sbm_tm.split('-')[2])) - datetime(int(x.time_order.split('-')[0]), int(x.time_order.split('-')[1]), int(x.time_order.split('-')[2]))).days, axis=1)\n",
    "    print(target_list)\n",
    "    day_mean = pd.pivot_table(target_list, index='id', values='days', aggfunc='mean').reset_index().rename(columns={'days': 'day_mean'})\n",
    "    day_max = pd.pivot_table(target_list, index='id', values='days', aggfunc='max').reset_index().rename(columns={'days': 'day_max'})\n",
    "    day_min = pd.pivot_table(target_list, index='id', values='days', aggfunc='min').reset_index().rename(columns={'days': 'day_min'})\n",
    "    order_record_count = pd.pivot_table(target_list, index='id', values='days', aggfunc='count').reset_index().rename(columns={'days': 'order_record_count'})\n",
    "    feature = feature.merge(day_mean, on='id', how='left')\n",
    "    feature = feature.merge(day_max, on='id', how='left')\n",
    "    feature = feature.merge(day_min, on='id', how='left')\n",
    "    feature = feature.merge(order_record_count, on='id', how='left')     # 记录数 #\n",
    "    feature.loc[:, 'order_record_unique'] = [1 if index == 1 else 0 for index in feature['order_record_count']]     # 记录数是否唯一 #\n",
    "    print(feature)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Logistic Regression'''\n",
    "def train_LR_module(store_result=False, store_feature=False, select_feature=False, feature_num='all', OneEncode=False):\n",
    "    train_feature = pd.read_csv('train_feature.csv', encoding='utf-8')\n",
    "    validate_feature = pd.read_csv('validate_feature.csv', encoding='utf-8')\n",
    "    test_feature = pd.read_csv('test_feature.csv', encoding='utf-8')\n",
    "    train_test_feature = pd.read_csv('train_test_feature.csv', encoding='utf-8')\n",
    "    print('读取数据完毕。。。')\n",
    "\n",
    "    validate_label = validate_feature[['target']]\n",
    "    train_label = train_feature[['target']]\n",
    "    train_test_label = train_test_feature[['target']]\n",
    "\n",
    "    train_feature = train_feature.iloc[:, 2:]\n",
    "    test_feature = test_feature.iloc[:, 1:]\n",
    "    validate_feature = validate_feature.iloc[:, 2:]\n",
    "    train_test_feature = train_test_feature.iloc[:, 2:]\n",
    "\n",
    "    if OneEncode is True:\n",
    "        features = list(train_feature.columns)\n",
    "        one_hot = []\n",
    "        continuous_feature = []\n",
    "        for name in features:\n",
    "            if len(set(train_feature[name])) == 2:\n",
    "                one_hot.append(name)\n",
    "            else:\n",
    "                continuous_feature.append(name)\n",
    "\n",
    "        feature = one_hot[:140] + continuous_feature\n",
    "        train_feature = train_feature[feature]\n",
    "        validate_feature = validate_feature[feature]\n",
    "        test_feature = test_feature[feature]\n",
    "        train_test_feature = train_test_feature[feature]\n",
    "\n",
    "    if select_feature is True:\n",
    "        print('开始特征选择。。。')\n",
    "        ch2 = SelectKBest(chi2, k=feature_num)\n",
    "        train_feature = ch2.fit_transform(train_feature, train_label)\n",
    "        test_feature = ch2.transform(test_feature)\n",
    "        validate_feature = ch2.transform(validate_feature)\n",
    "        train_test_feature = ch2.transform(train_test_feature)\n",
    "        print('特征选择完毕。。。')\n",
    "    else:\n",
    "        feature_num = train_feature.shape[1]\n",
    "\n",
    "    print('开始训练logisticRegression模型。。。')\n",
    "    module = LogisticRegression(penalty='l2', solver='sag', max_iter=500, random_state=42, n_jobs=4)  # , solver='sag'\n",
    "    # module = lgb.LGBMClassifier(\n",
    "    #     num_leaves=64,  # num_leaves = 2^max_depth * 0.6 #\n",
    "    #     max_depth=6,\n",
    "    #     n_estimators=80,\n",
    "    #     learning_rate=0.1\n",
    "    # )\n",
    "    '''训练集'''\n",
    "    module.fit(train_feature, train_label)\n",
    "\n",
    "    if store_result is True:\n",
    "        '''测试训练集'''\n",
    "        module_two = LogisticRegression(penalty='l2', solver='sag', max_iter=500, random_state=42, n_jobs=4)\n",
    "        # module_two = lgb.LGBMClassifier(\n",
    "        #     num_leaves=64,  # num_leaves = 2^max_depth * 0.6 #\n",
    "        #     max_depth=6,\n",
    "        #     n_estimators=80,\n",
    "        #     learning_rate=0.1\n",
    "        # )\n",
    "        module_two.fit(train_test_feature, train_test_label)\n",
    "\n",
    "        result = module_two.predict_proba(test_feature)[:, 1]\n",
    "        result = pd.DataFrame(result)\n",
    "        result.columns = ['predicted_score']\n",
    "        sample = test_list[['id']]\n",
    "        sample['predicted_score'] = [index for index in result['predicted_score']]\n",
    "        sample.columns = ['ID', 'PROB']\n",
    "        sample.to_csv('lr_sample.csv', index=None)\n",
    "        # sample.to_csv(r'lgb_sample.csv', index=None)\n",
    "        print(sample)\n",
    "        print('结果已更新。。。')\n",
    "\n",
    "    print(\" Score_offline:\", roc_auc_score(validate_label, module.predict_proba(validate_feature)[:, 1]))\n",
    "    print('特征维数：', feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelection(object):\n",
    "    def __init__(self, feature_num):\n",
    "        self.feature_num = feature_num\n",
    "        self.train_test, self.label, self.test = self.read_data()    # features #\n",
    "        self.feature_name = list(self.train_test.columns)     # feature name #\n",
    "\n",
    "    def read_data(self):\n",
    "        test = pd.read_csv('test_feature.csv', encoding='utf-8')\n",
    "        train_test = pd.read_csv('train_test_feature.csv', encoding='utf-8')\n",
    "        train_test = train_test.drop(['feature_1', 'register_days', 'id_card_one', 'id_card_two', 'id_card_three', 'id_card_four', 'id_card_five', 'id_card_six', 'mobile', 'unicom', 'telecom', 'virtual'], axis=1)\n",
    "        print('读取数据完毕。。。')\n",
    "        label = train_test[['target']]\n",
    "        test = test.iloc[:, 1:]\n",
    "        train_test = train_test.iloc[:, 2:]\n",
    "        return train_test, label, test\n",
    "\n",
    "    def variance_threshold(self):\n",
    "        sel = VarianceThreshold()\n",
    "        sel.fit_transform(self.train_test)\n",
    "        feature_var = list(sel.variances_)    # feature variance #\n",
    "        features = dict(zip(self.feature_name, feature_var))\n",
    "        features = list(dict(sorted(features.items(), key=lambda d: d[1])).keys())[-self.feature_num:]\n",
    "        # print(features)   # 100 cols #\n",
    "        return set(features)   # return set type #\n",
    "\n",
    "    def select_k_best(self):\n",
    "        ch2 = SelectKBest(chi2, k=self.feature_num)\n",
    "        ch2.fit(self.train_test, self.label)\n",
    "        feature_var = list(ch2.scores_)  # feature scores #\n",
    "        features = dict(zip(self.feature_name, feature_var))\n",
    "        features = list(dict(sorted(features.items(), key=lambda d: d[1])).keys())[-self.feature_num:]\n",
    "        # print(features)     # 100 cols #\n",
    "        return set(features)    # return set type #\n",
    "\n",
    "    def svc_select(self):\n",
    "        svc = SVC(kernel='rbf', C=1, random_state=2018)    # linear #\n",
    "        rfe = RFE(estimator=svc, n_features_to_select=self.feature_num, step=1)\n",
    "        rfe.fit(self.train_test, self.label.ravel())\n",
    "        print(rfe.ranking_)\n",
    "        return rfe.ranking_\n",
    "\n",
    "    def tree_select(self):\n",
    "        clf = ExtraTreesClassifier(n_estimators=300, max_depth=7, n_jobs=4)\n",
    "        clf.fit(self.train_test, self.label)\n",
    "        feature_var = list(clf.feature_importances_)  # feature scores #\n",
    "        features = dict(zip(self.feature_name, feature_var))\n",
    "        features = list(dict(sorted(features.items(), key=lambda d: d[1])).keys())[-self.feature_num:]\n",
    "        # print(features)     # 100 cols #\n",
    "        return set(features)  # return set type #\n",
    "\n",
    "    def return_feature_set(self, variance_threshold=False, select_k_best=False, svc_select=False, tree_select=False):\n",
    "        names = set([])\n",
    "        if variance_threshold is True:\n",
    "            name_one = self.variance_threshold()\n",
    "            names = names.union(name_one)\n",
    "        if select_k_best is True:\n",
    "            name_two = self.select_k_best()\n",
    "            names = names.intersection(name_two)\n",
    "        if svc_select is True:\n",
    "            name_three = self.svc_select()\n",
    "            names = names.intersection(name_three)\n",
    "        if tree_select is True:\n",
    "            name_four = self.tree_select()\n",
    "            names = names.intersection(name_four)\n",
    "\n",
    "        # print(len(names))\n",
    "        print(names)\n",
    "        return list(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_module(features_name, store_result=False):\n",
    "    '''训练模型'''\n",
    "    train_feature = pd.read_csv('train_feature.csv', encoding='utf-8')\n",
    "    validate_feature = pd.read_csv('validate_feature.csv', encoding='utf-8')\n",
    "    test_feature = pd.read_csv('test_feature.csv', encoding='utf-8')\n",
    "    train_test_feature = pd.read_csv('train_test_feature.csv', encoding='utf-8')\n",
    "\n",
    "    print('读取数据完毕。。。')\n",
    "\n",
    "    validate_label = validate_feature[['target']]\n",
    "    train_label = train_feature[['target']]\n",
    "    train_test_label = train_test_feature[['target']]\n",
    "\n",
    "    train_feature = train_feature[features_name]\n",
    "    test_feature = test_feature[features_name]\n",
    "    validate_feature = validate_feature[features_name]\n",
    "    train_test_feature = train_test_feature[features_name]\n",
    "\n",
    "    print('开始训练xgboost模型。。。')\n",
    "    '''xgboost分类器'''\n",
    "    num_round = 500    # 迭代次数 #\n",
    "    params = {\n",
    "        'booster': 'gbtree',\n",
    "        'max_depth': 4,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'subsample': 0.8,\n",
    "        'eta': 0.03,\n",
    "        'silent': 1,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'min_child_weight': 1,\n",
    "        'scale_pos_weight': 1,\n",
    "        'seed': 27,\n",
    "        'reg_alpha': 0.01\n",
    "    }\n",
    "    '''训练集'''\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    validate_feature = xgb.DMatrix(validate_feature)\n",
    "    module = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "    if store_result is True:\n",
    "        '''测试训练集'''\n",
    "        dtrain_two = xgb.DMatrix(train_test_feature, label=train_test_label)\n",
    "        test_feature = xgb.DMatrix(test_feature)\n",
    "        module_two = xgb.train(params, dtrain_two, num_round)\n",
    "\n",
    "        features = module_two.get_fscore()\n",
    "        features = list(dict(sorted(features.items(), key=lambda d: d[1])).keys())[-20:]\n",
    "        features.reverse()\n",
    "        print(features)       # 输出特征重要性 #\n",
    "\n",
    "        result = module_two.predict(test_feature)\n",
    "        result = pd.DataFrame(result)\n",
    "        result.columns = ['predicted_score']\n",
    "        test_list = pd.read_csv('../dataset/AI_Risk_BtestData_V1.0/Btest_list.csv', low_memory=False)\n",
    "        sample = test_list[['id']]\n",
    "        sample['predicted_score'] = [index for index in result['predicted_score']]\n",
    "        sample.columns = ['ID', 'PROB']\n",
    "        sample.to_csv(r'xgb_sample.csv', index=None)\n",
    "        print(sample)\n",
    "        print('结果已更新。。。')\n",
    "\n",
    "    print(\" Score_offline:\", roc_auc_score(validate_label, module.predict(validate_feature)))\n",
    "    print('特征维数：', len(features_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lr_module(features_name, store_result=False):\n",
    "    train_feature = pd.read_csv(r'train_feature.csv', encoding='utf-8')\n",
    "    validate_feature = pd.read_csv(r'validate_feature.csv', encoding='utf-8')\n",
    "    test_feature = pd.read_csv(r'test_feature.csv', encoding='utf-8')\n",
    "    train_test_feature = pd.read_csv(r'train_test_feature.csv', encoding='utf-8')\n",
    "    print('读取数据完毕。。。')\n",
    "\n",
    "    validate_label = validate_feature[['target']]\n",
    "    train_label = train_feature[['target']]\n",
    "    train_test_label = train_test_feature[['target']]\n",
    "\n",
    "    train_feature = train_feature[features_name]\n",
    "    test_feature = test_feature[features_name]\n",
    "    validate_feature = validate_feature[features_name]\n",
    "    train_test_feature = train_test_feature[features_name]\n",
    "\n",
    "    print('开始训练logisticRegression模型。。。')\n",
    "    module = LogisticRegression(penalty='l2', solver='sag', max_iter=500, random_state=42, n_jobs=4)  # , solver='sag'\n",
    "    # module = lgb.LGBMClassifier(\n",
    "    #     num_leaves=64,  # num_leaves = 2^max_depth * 0.6 #\n",
    "    #     max_depth=6,\n",
    "    #     n_estimators=80,\n",
    "    #     learning_rate=0.1\n",
    "    # )\n",
    "    '''训练集'''\n",
    "    module.fit(train_feature, train_label)\n",
    "\n",
    "    if store_result is True:\n",
    "        '''测试训练集'''\n",
    "        module_two = LogisticRegression(\n",
    "            penalty='l2',\n",
    "            solver='sag',\n",
    "            max_iter=500,\n",
    "            random_state=42,\n",
    "            n_jobs=4\n",
    "        )\n",
    "\n",
    "        # module_two = lgb.LGBMClassifier(\n",
    "        #     num_leaves=64,  # num_leaves = 2^max_depth * 0.6 #\n",
    "        #     max_depth=6,\n",
    "        #     n_estimators=80,\n",
    "        #     learning_rate=0.1\n",
    "        # )\n",
    "        module_two.fit(train_test_feature, train_test_label)\n",
    "\n",
    "        result = module_two.predict_proba(test_feature)[:, 1]\n",
    "        result = pd.DataFrame(result)\n",
    "        result.columns = ['predicted_score']\n",
    "        test_list = pd.read_csv('../dataset/AI_Risk_BtestData_V1.0/Btest_list.csv', low_memory=False)\n",
    "        sample = test_list[['id']]\n",
    "        sample['predicted_score'] = [index for index in result['predicted_score']]\n",
    "        sample.columns = ['ID', 'PROB']\n",
    "        sample.to_csv(r'lr_sample.csv', index=None)\n",
    "        # sample.to_csv(r'lgb_sample.csv', index=None)\n",
    "        print(sample)\n",
    "        print('结果已更新。。。')\n",
    "\n",
    "    print(\" Score_offline:\", roc_auc_score(validate_label, module.predict_proba(validate_feature)[:, 1]))\n",
    "    print('特征维数：', len(features_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''划分数据集'''\n",
    "train_target['date'] = [index.replace('-', '') for index in train_target['appl_sbm_tm']]\n",
    "train_target['date'] = [index.split(' ')[0][0:6] for index in train_target['date']]\n",
    "'''验证集'''\n",
    "validate_data = train_target[(train_target['date'] == '201704')][['target', 'id']]\n",
    "'''训练集'''\n",
    "train_data = train_target[(train_target['date'] >= '201603') & (train_target['date'] <= '201703')][['target', 'id']]\n",
    "'''测试集'''\n",
    "test_data = test_list[['id']]\n",
    "'''测试训练集'''\n",
    "train_test_data = train_target[['target', 'id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature():\n",
    "    '''credit_info'''\n",
    "    train_credit_info_feature = extract_credit_info(train_credit_info)\n",
    "    train_test_feature = train_test_data.merge(train_credit_info_feature, on='id', how='left')    # 训练测试集 #\n",
    "    train_feature = train_data.merge(train_credit_info_feature, on='id', how='left')\n",
    "    validate_feature = validate_data.merge(train_credit_info_feature, on='id', how='left')\n",
    "    test_feature = test_data.merge(extract_credit_info(test_credit_info), on='id', how='left')\n",
    "\n",
    "    '''order_info'''\n",
    "    train_order_info_feature = extract_order_info(train_order_info)\n",
    "    train_feature = train_feature.merge(train_order_info_feature, on='id', how='left')\n",
    "    train_test_feature = train_test_feature.merge(train_order_info_feature, on='id', how='left')  # 训练测试集 #\n",
    "    validate_feature = validate_feature.merge(train_order_info_feature, on='id', how='left')\n",
    "    test_feature = test_feature.merge(extract_order_info(test_order_info), on='id', how='left')\n",
    "\n",
    "    '''user_info'''\n",
    "    train_user_info_feature = extract_user_info(train_user_info)\n",
    "    train_feature = train_feature.merge(train_user_info_feature, on='id', how='left')\n",
    "    train_test_feature = train_test_feature.merge(train_user_info_feature, on='id', how='left')    # 训练测试集 #\n",
    "    validate_feature = validate_feature.merge(train_user_info_feature, on='id', how='left')\n",
    "    test_feature = test_feature.merge(extract_user_info(test_user_info), on='id', how='left')\n",
    "\n",
    "    '''recieve_addr_info'''\n",
    "    train_recieve_addr_info_feature = extract_recieve_addr_info(train_recieve_addr_info)\n",
    "    train_feature = train_feature.merge(train_recieve_addr_info_feature, on='id', how='left')\n",
    "    train_test_feature = train_test_feature.merge(train_recieve_addr_info_feature, on='id', how='left')  # 训练测试集 #\n",
    "    validate_feature = validate_feature.merge(train_recieve_addr_info_feature, on='id', how='left')\n",
    "    test_feature = test_feature.merge(extract_recieve_addr_info(test_recieve_addr_info), on='id', how='left')\n",
    "\n",
    "    '''bankcard_info'''\n",
    "    train_bankcard_info_feature = extract_bankcard_info(train_bankcard_info)\n",
    "    train_feature = train_feature.merge(train_bankcard_info_feature, on='id', how='left')\n",
    "    train_test_feature = train_test_feature.merge(train_bankcard_info_feature, on='id', how='left')  # 训练测试集 #\n",
    "    validate_feature = validate_feature.merge(train_bankcard_info_feature, on='id', how='left')\n",
    "    test_feature = test_feature.merge(extract_bankcard_info(test_bankcard_info), on='id', how='left')\n",
    "\n",
    "    '''auth_info'''\n",
    "    train_auth_info_feature = extract_auth_info(train_auth_info)\n",
    "    train_feature = train_feature.merge(train_auth_info_feature, on='id', how='left').fillna(0)\n",
    "    train_test_feature = train_test_feature.merge(train_auth_info_feature, on='id', how='left').fillna(0)  # 训练测试集 #\n",
    "    validate_feature = validate_feature.merge(train_auth_info_feature, on='id', how='left').fillna(0)\n",
    "    test_feature = test_feature.merge(extract_auth_info(test_auth_info), on='id', how='left').fillna(0)\n",
    "\n",
    "    '''time relative features one'''\n",
    "    train_time_feature = extract_time_feature(train_auth_info, train_target)\n",
    "    train_feature = train_feature.merge(train_time_feature, on='id', how='left').fillna(0)\n",
    "    train_test_feature = train_test_feature.merge(train_time_feature, on='id', how='left').fillna(0)  # 训练测试集 #\n",
    "    validate_feature = validate_feature.merge(train_time_feature, on='id', how='left').fillna(0)\n",
    "    test_feature = test_feature.merge(extract_time_feature(test_auth_info, test_list), on='id', how='left').fillna(0)\n",
    "\n",
    "    '''time relative features two'''\n",
    "    train_order_payment_time = extract_order_payment_time(train_order_info, train_target)\n",
    "    train_feature = train_feature.merge(train_order_payment_time, on='id', how='left').fillna(0)\n",
    "    train_test_feature = train_test_feature.merge(train_order_payment_time, on='id', how='left').fillna(0)  # 训练测试集 #\n",
    "    validate_feature = validate_feature.merge(train_order_payment_time, on='id', how='left').fillna(0)\n",
    "    test_feature = test_feature.merge(extract_order_payment_time(test_order_info, test_list), on='id', how='left').fillna(0)\n",
    "\n",
    "    print(train_feature.head(5))\n",
    "    print(validate_feature.head(5))\n",
    "    print(test_feature.head(5))\n",
    "    return train_feature, validate_feature, test_feature, train_test_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_module(store_result=False, store_feature=False, select_feature=False, feature_num='all', one_encode=False):\n",
    "    '''训练模型'''\n",
    "    if store_feature is True:\n",
    "        train_feature, validate_feature, test_feature, train_test_feature = extract_feature()\n",
    "        ''' 保存特征数据 '''\n",
    "        train_feature.to_csv(r'train_feature.csv', index=None, encoding='utf-8')\n",
    "        validate_feature.to_csv(r'validate_feature.csv', index=None, encoding='utf-8')\n",
    "        test_feature.to_csv(r'test_feature.csv', index=None, encoding='utf-8')\n",
    "        train_test_feature.to_csv(r'train_test_feature.csv', index=None, encoding='utf-8')\n",
    "        print('保存数据完毕。。。')\n",
    "\n",
    "        print('特征提取完毕。。。')\n",
    "        exit(0)\n",
    "    else:\n",
    "        train_feature = pd.read_csv(r'train_feature.csv', encoding='utf-8')\n",
    "        validate_feature = pd.read_csv(r'validate_feature.csv', encoding='utf-8')\n",
    "        test_feature = pd.read_csv(r'test_feature.csv', encoding='utf-8')\n",
    "        train_test_feature = pd.read_csv(r'train_test_feature.csv', encoding='utf-8')\n",
    "        print('读取数据完毕。。。')\n",
    "\n",
    "    validate_label = validate_feature[['target']]\n",
    "    train_label = train_feature[['target']]\n",
    "    train_test_label = train_test_feature[['target']]\n",
    "\n",
    "    train_feature = train_feature.iloc[:, 2:]\n",
    "    test_feature = test_feature.iloc[:, 1:]\n",
    "    validate_feature = validate_feature.iloc[:, 2:]\n",
    "    train_test_feature = train_test_feature.iloc[:, 2:]\n",
    "\n",
    "    train_feature = train_feature.drop(['feature_1', 'register_days'], axis=1)\n",
    "    test_feature = test_feature.drop(['feature_1', 'register_days'], axis=1)\n",
    "    validate_feature = validate_feature.drop(['feature_1', 'register_days'], axis=1)\n",
    "    train_test_feature = train_test_feature.drop(['feature_1', 'register_days'], axis=1)\n",
    "\n",
    "    if one_encode is True:\n",
    "        features = list(train_feature.columns)\n",
    "        continuous_feature = []\n",
    "        one_hot = []\n",
    "        for name in features:\n",
    "            if len(set(train_feature[name])) != 2:\n",
    "                continuous_feature.append(name)\n",
    "            else:\n",
    "                one_hot.append(name)\n",
    "\n",
    "        feature = continuous_feature + one_hot[:130]\n",
    "        train_feature = train_feature[feature]\n",
    "        validate_feature = validate_feature[feature]\n",
    "        test_feature = test_feature[feature]\n",
    "        train_test_feature = train_test_feature[feature]\n",
    "\n",
    "    if select_feature is True:\n",
    "        print('开始特征选择。。。')\n",
    "        ch2 = SelectKBest(chi2, k=feature_num)\n",
    "        train_feature = ch2.fit_transform(train_feature, train_label)\n",
    "        test_feature = ch2.transform(test_feature)\n",
    "        validate_feature = ch2.transform(validate_feature)\n",
    "        train_test_feature = ch2.transform(train_test_feature)\n",
    "        print('特征选择完毕。。。')\n",
    "    else:\n",
    "        feature_num = train_feature.shape[1]\n",
    "\n",
    "    print('开始训练xgboost模型。。。')\n",
    "    '''xgboost分类器'''\n",
    "    num_round = 500    # 迭代次数 #\n",
    "    params = {\n",
    "        'booster': 'gbtree',\n",
    "        'max_depth': 4,\n",
    "        'colsample_bytree': 0.6,\n",
    "        'subsample': 0.7,\n",
    "        'eta': 0.03,\n",
    "        'silent': 1,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        # 'min_child_weight': 1,\n",
    "        'scale_pos_weight': 1,\n",
    "        # 'seed': 27,\n",
    "        # 'reg_alpha': 0.01\n",
    "    }\n",
    "    '''训练集'''\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    validate_feature = xgb.DMatrix(validate_feature)\n",
    "    module = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "    if store_result is True:\n",
    "        '''测试训练集'''\n",
    "        dtrain_two = xgb.DMatrix(train_test_feature, label=train_test_label)\n",
    "        test_feature = xgb.DMatrix(test_feature)\n",
    "        module_two = xgb.train(params, dtrain_two, num_round)\n",
    "\n",
    "        result = module_two.predict(test_feature)\n",
    "        result = pd.DataFrame(result)\n",
    "        result.columns = ['predicted_score']\n",
    "        sample = test_list[['id']]\n",
    "        sample['predicted_score'] = [index for index in result['predicted_score']]\n",
    "        sample.columns = ['ID', 'PROB']\n",
    "        sample.to_csv(r'xgb_sample.csv', index=None)\n",
    "        print(sample)\n",
    "        print('结果已更新。。。')\n",
    "\n",
    "    print(\" Score_offline:\", roc_auc_score(validate_label, module.predict(validate_feature)))\n",
    "    print('特征维数：', feature_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 模型融合 '''\n",
    "def module_merge_triple(prob_xgb, prob_lr, prob_lgb):\n",
    "    xgb_sample = pd.read_csv(r'result_xgb.csv', low_memory=False)   # encode:159:0.790297834417\n",
    "    lr_sample = pd.read_csv(r'lr_sample.csv', low_memory=False)     # Uncode:0.792171452209\n",
    "    lgb_sample = pd.read_csv(r'xgb_sample_51.csv', low_memory=False)\n",
    "\n",
    "    xgb_sample.columns = ['ID', 'PROB_xgb']\n",
    "    lr_sample.columns = ['ID', 'PROB_lr']\n",
    "    lgb_sample.columns = ['ID', 'PROB_lgb']\n",
    "    sample = xgb_sample.merge(lr_sample, on='ID', how='left')\n",
    "    sample = sample.merge(lgb_sample, on='ID', how='left')\n",
    "    # print(sample)\n",
    "    sample['PROB'] = sample['PROB_xgb'] * prob_xgb + sample['PROB_lr'] * prob_lr + sample['PROB_lgb'] * prob_lgb\n",
    "    sample = sample[['ID', 'PROB']]\n",
    "    print(sample)\n",
    "    sample.to_csv(r'sample.csv', index=None)\n",
    "    print('模型已融合。。。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def module_merge_double(prob_x, prob_l):\n",
    "    xgb_sample = pd.read_csv(r'result0501_152.csv', low_memory=False)   # encode:159:0.790297834417\n",
    "    lr_sample = pd.read_csv(r'xgb_sample_51.csv', low_memory=False)     # Uncode:0.792171452209\n",
    "    sample = xgb_sample.merge(lr_sample, on='ID', how='left')\n",
    "    sample['PROB'] = sample['PROB_x'] * prob_x + sample['PROB_y'] * prob_l\n",
    "    sample = sample[['ID', 'PROB']]\n",
    "    print(sample)\n",
    "    sample.to_csv(r'sample.csv', index=None)\n",
    "    print('模型已融合。。。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''xgboost单模型'''\n",
    "    train_module(store_result=False, store_feature=True, select_feature=False, feature_num='all', one_encode=False)\n",
    "\n",
    "    '''LogisticRegression单模型'''\n",
    "    # train_LR_module(store_result=False, select_feature=True, feature_num=140, OneEncode=False)\n",
    "    '''线性融合三个sample'''\n",
    "    # module_merge_triple(prob_xgb=0.4, prob_lr=0.2, prob_lgb=0.4)\n",
    "    '''现行融合两个sample'''\n",
    "    # module_merge_double(prob_x=0.5, prob_l=0.5)\n",
    "    '''Stacking'''\n",
    "    # # ensemble = Ensemble(5, xgb_module, [xgb_module, lgb_module, lr_module, rf_module, gb_module])\n",
    "    # ensemble = Ensemble(4, lr_module, [xgb_module, xgb_module, xgb_module, xgb_module])\n",
    "    # train_test, label, test = ensemble.read_data()\n",
    "    # result = ensemble.fit_predict(train_test, label, test)\n",
    "    # print('模型融合完毕。。。')\n",
    "    # result = pd.DataFrame(result, columns=['PROB'])\n",
    "    # sample = pd.read_csv(r'lr_sample.csv', low_memory=False)\n",
    "    # sample['PROB'] = [index for index in result['PROB']]\n",
    "    # sample.to_csv(r'stacking.csv', index=None)\n",
    "    # print(sample)\n",
    "    # print('数据整合完毕。。。')\n",
    "\n",
    "    '''multiply_feature_selection  xgboost_module'''\n",
    "    features_name = ['order_all_is_null', 'feature_1', 'register_days', 'quota', 'quota_surplus',  'all_is_null_y',  'account_grade_is_null', 'all_is_zero', 'account_grade2', 'age_three', 'type_pay_len', 'null_y', '等待付款', 'income1', 'auth_time_is_null', 'record_count', 'qq_bound_is_null', 'card_record_count', 'quota_is_zero', '新疆', '云南', 'account_grade3', '广东', 'card_time_is_null', 'have_credit_card', '充值成功', '已取消', 'credit_count', '在线', '四川', 'wechat_bound_is_null', 'null', 'credit_score_rank', '未抢中', 'null_x', '完成', '天津', 'age_two', 'female', '订单取消', 'quota_rate', '山东', '重庆', 'sts_order_len', 'merriage1', '福建', 'account_grade1', 'phone_count', 'record_is_unique', '上海', 'income3', '湖北', 'phone_is_null', 'time_phone_is_null', 'province_len', 'birthday_is_zero', '混合支付', 'auth_id_card_is_null', 'credit_score', '江西', '货到付款', '吉林', 'credit_score_is_null', '江苏', 'all_not_null', 'sex_secret', '已完成', 'card_category_count', 'card_count_one', '等待收货', '湖南', 'male', 'store_card_count']\n",
    "    train_xgb_module(features_name, store_result=True)\n",
    "\n",
    "    # 0.81882083452 seed=27\n",
    "    # original   ->    0.816853963449\n",
    "    # colsample_bytree: 0.8   ->  0.818427843445\n",
    "    # scale_pos_weight: 16   ->   0.82029535496\n",
    "    # reg_alpha: 0.01  ->   0.820431061402\n",
    "    # 'quota', 'quota_surplus',  ->   0.820543215061\n",
    "\n",
    "    '''multiply_feature_selection  LogisticRegression_module'''\n",
    "    features_name = ['order_all_is_null', 'feature_1', 'record_is_unique', '浙江', '辽宁', 'card_time_is_null', 'income1', 'account_grade2', '黑龙', '江苏', '未抢中', '山东', '内蒙', '上海', '分期付款', '货到付款', 'overdraft', '公司转账', 'null', '订单取消', 'age_two', '充值成功', '在线', '新疆', '完成', 'quota_rate', 'sex_not_male', '湖北', 'quota', 'account_grade_is_null', '安徽', 'card_category_count', 'all_not_null', 'phone_is_null', '河北', 'merriage_is_null', '混合支付', 'quota_surplus_is_null', 'birthday_is_zero', 'income3', '江西', 'store_card_count', 'time_phone_is_null', 'id_card_is_null', 'auth_id_card_is_null', '已取消', '广东', 'record_count', '云南', '等待付款', '已完成', 'card_count_one', 'type_pay_len', 'female', 'sts_order_len', '福建', 'auth_time_is_null', '在线支付', 'null_x', 'income2', 'quota_is_zero', 'credit_score_is_null', 'account_grade3', '四川', '等待审核', '重庆', '河南', 'all_is_null_y', '吉林', '抢票已取消', 'province_len', 'credit_count', 'account_grade1', 'credit_score_rank', 'sts_order_count', '湖南', '充值失败;退款成功', 'wechat_bound_is_null', 'card_record_count', 'male', '邮局汇款', 'merriage1', '山西', 'phone_count', 'sex_secret', '海南', 'merriage2', '等待收货', 'all_is_zero', '天津', 'credit_score', 'age_three', 'null_y', 'qq_bound_is_null', 'have_credit_card', '北京']\n",
    "    train_lr_module(features_name, store_result=True)\n",
    "\n",
    "    module_merge_triple(prob_xgb=0.4, prob_lr=0.2, prob_lgb=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rice/.conda/envs/xgb/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/rice/.conda/envs/xgb/lib/python3.7/site-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n",
      "/home/rice/.conda/envs/xgb/lib/python3.7/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/home/rice/.conda/envs/xgb/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b4adb78d6cbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'程序耗时：'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-cb9af5fb40af>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m'''xgboost单模型'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_encode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'''LogisticRegression单模型'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-e0afa3b6772f>\u001b[0m in \u001b[0;36mtrain_module\u001b[0;34m(store_result, store_feature, select_feature, feature_num, one_encode)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m'''训练模型'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstore_feature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_test_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;34m''' 保存特征数据 '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtrain_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'train_feature.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-2fa7356de1dc>\u001b[0m in \u001b[0;36mextract_feature\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtrain_test_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_recieve_addr_info_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 训练测试集 #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mvalidate_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_recieve_addr_info_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtest_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_recieve_addr_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_recieve_addr_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m'''bankcard_info'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-8927cf77b2a9>\u001b[0m in \u001b[0;36mextract_recieve_addr_info\u001b[0;34m(recieve_addr_info)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mcity_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'province_len'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcity_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'province'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'province'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m# print(feature)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/xgb/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   6866\u001b[0m                      \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6867\u001b[0m                      \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6868\u001b[0;31m                      copy=copy, indicator=indicator, validate=validate)\n\u001b[0m\u001b[1;32m   6869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6870\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/xgb/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     45\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                          validate=validate)\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/xgb/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# to avoid incompat dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;31m# If argument passed to validate,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/xgb/lib/python3.7/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    978\u001b[0m                       (inferred_right in string_types and\n\u001b[1;32m    979\u001b[0m                        inferred_left not in string_types)):\n\u001b[0;32m--> 980\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0;31m# datetimelikes must match exactly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "main()\n",
    "end_time = time.clock()\n",
    "print('程序耗时：', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgb",
   "language": "python",
   "name": "xgb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
