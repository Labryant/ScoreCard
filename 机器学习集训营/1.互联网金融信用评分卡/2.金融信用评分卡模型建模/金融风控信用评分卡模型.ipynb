{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/chinahadoop.png)\n",
    "# 金融风控信用评分卡模型\n",
    "**[小象学院](http://www.chinahadoop.cn/course/landpage/15)《机器学习集训营》课程资料 by [@寒小阳](https://www.chinahadoop.cn/user/49339/about)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 背景介绍\n",
    "信用风险计量体系包括主体评级模型和债项评级两部分。\n",
    "\n",
    "主体评级和债项评级均有一系列评级模型组成，其中主体评级模型可用“四张卡”来表示，分别是A卡、B卡、C卡和F卡；债项评级模型通常按照主体的融资用途，分为企业融资模型、现金流融资模型和项目融资模型等。 我们主要讨论主体评级模型的开发过程。\n",
    "\n",
    "## 项目流程\n",
    "\n",
    "典型的信用评分模型如图1-1所示。信用风险评级模型的主要开发流程如下：\n",
    "- （1） 数据获取，包括获取存量客户及潜在客户的数据。存量客户是指已经在证券公司开展相关融资类业务的客户，包括个人客户和机构客户；潜在客户是指未来拟在证券公司开展相关融资类业务的客户，主要包括机构客户，这也是解决证券业样本较少的常用方法，这些潜在机构客户包括上市公司、公开发行债券的发债主体、新三板上市公司、区域股权交易中心挂牌公司、非标融资机构等。\n",
    "- （2） 数据预处理，主要工作包括数据清洗、缺失值处理、异常值处理，主要是为了将获取的原始数据转化为可用作模型开发的格式化数据。\n",
    "- （3） 探索性数据分析，该步骤主要是获取样本总体的大概情况，描述样本总体情况的指标主要有直方图、箱形图等。\n",
    "- （4） 变量选择，该步骤主要是通过统计学的方法，筛选出对违约状态影响最显著的指标。主要有单变量特征选择方法和基于机器学习模型的方法 。\n",
    "- （5） 模型开发，该步骤主要包括变量分段、变量的WOE（证据权重）变换和逻辑回归估算三部分。\n",
    "- （6） 模型评估，该步骤主要是评估模型的区分能力、预测能力、稳定性，并形成模型评估报告，得出模型是否可以使用的结论。\n",
    "- （7） 信用评分，根据逻辑回归的系数和WOE等确定信用评分的方法。将Logistic模型转换为标准评分的形式。\n",
    "- （8） 建立评分系统，根据信用评分方法，建立自动信用评分系统。\n",
    "\n",
    "![img](./img/7744536-636aa1632b489f73.png)\n",
    "\n",
    "PS：有些时候为了便于命名，相应的变量用标号代替\n",
    "\n",
    "## 数据获取\n",
    "\n",
    "数据来自Kaggle的[Give Me Some Credit](https://link.jianshu.com?t=https://www.kaggle.com/c/GiveMeSomeCredit/data)，有15万条的样本数据，下图可以看到这份数据的大致情况。\n",
    "数据属于个人消费类贷款，只考虑信用评分最终实施时能够使用到的数据应从如下一些方面获取数据：\n",
    "- 基本属性：包括了借款人当时的年龄。\n",
    "- 偿债能力：包括了借款人的月收入、负债比率。\n",
    "- 信用往来：两年内35-59天逾期次数、两年内60-89天逾期次数、两年内90\n",
    "天或高于90天逾期的次数。\n",
    "- 财产状况：包括了开放式信贷和贷款数量、不动产贷款或额度数量。\n",
    "- 贷款属性：暂无。\n",
    "- 其他因素：包括了借款人的家属数量（不包括本人在内）。\n",
    "- 时间窗口：自变量的观察窗口为过去两年，因变量表现窗口为未来两年。\n",
    "\n",
    "![img](./img/7744536-11bd43ba74266260.png)\n",
    "\n",
    "## 数据预处理\n",
    "\n",
    "在对数据处理之前，需要对数据的缺失值和异常值情况进行了解。Python内有describe()函数，可以了解数据集的缺失值、均值和中位数等。\n",
    "\n",
    "```python\n",
    "    #载入数据\n",
    "    data = pd.read_csv('cs-training.csv')\n",
    "    #数据集确实和分布情况\n",
    "    data.describe().to_csv('DataDescribe.csv')\n",
    "```\n",
    "\n",
    "数据集的详细情况：\n",
    "\n",
    "![img](./img/7744536-4b22343f1067c47e.png)\n",
    "\n",
    "从上图可知，变量MonthlyIncome和NumberOfDependents存在缺失，变量MonthlyIncome共有缺失值29731个，NumberOfDependents有3924个缺失值。\n",
    "\n",
    "### 缺失值处理\n",
    "\n",
    "这种情况在现实问题中非常普遍，这会导致一些不能处理缺失值的分析方法无法应用，因此，在信用风险评级模型开发的第一步我们就要进行缺失值处理。缺失值处理的方法，包括如下几种。\n",
    "- （1） 直接删除含有缺失值的样本。\n",
    "- （2） 根据样本之间的相似性填补缺失值。\n",
    "- （3） 根据变量之间的相关关系填补缺失值。\n",
    "变量MonthlyIncome缺失率比较大，所以我们根据变量之间的相关关系填补缺失值，我们采用随机森林法：\n",
    "\n",
    "```python\n",
    "# 用随机森林对缺失值预测填充函数\n",
    "def set_missing(df):\n",
    "    # 把已有的数值型特征取出来\n",
    "    process_df = df.ix[:,[5,0,1,2,3,4,6,7,8,9]]\n",
    "    # 分成已知该特征和未知该特征两部分\n",
    "    known = process_df[process_df.MonthlyIncome.notnull()].as_matrix()\n",
    "    unknown = process_df[process_df.MonthlyIncome.isnull()].as_matrix()\n",
    "    # X为特征属性值\n",
    "    X = known[:, 1:]\n",
    "    # y为结果标签值\n",
    "    y = known[:, 0]\n",
    "    # fit到RandomForestRegressor之中\n",
    "    rfr = RandomForestRegressor(random_state=0, \n",
    "    n_estimators=200,max_depth=3,n_jobs=-1)\n",
    "    rfr.fit(X,y)\n",
    "    # 用得到的模型进行未知特征值预测\n",
    "    predicted = rfr.predict(unknown[:, 1:]).round(0)\n",
    "    print(predicted)\n",
    "    # 用得到的预测结果填补原缺失数据\n",
    "    df.loc[(df.MonthlyIncome.isnull()), 'MonthlyIncome'] = predicted\n",
    "    return df\n",
    "```\n",
    "\n",
    "NumberOfDependents变量缺失值比较少，直接删除，对总体模型不会造成太大影响。对缺失值处理完之后，删除重复项。\n",
    "\n",
    "```python\n",
    "    data=set_missing(data)#用随机森林填补比较多的缺失值\n",
    "    data=data.dropna()#删除比较少的缺失值\n",
    "    data = data.drop_duplicates()#删除重复项\n",
    "    data.to_csv('MissingData.csv',index=False)\n",
    "```\n",
    "\n",
    "### 异常值处理\n",
    "\n",
    "缺失值处理完毕后，我们还需要进行异常值处理。异常值是指明显偏离大多数抽样数据的数值，比如个人客户的年龄为0时，通常认为该值为异常值。找出样本总体中的异常值，通常采用离群值检测的方法。\n",
    "首先，我们发现变量age中存在0，显然是异常值，直接剔除：\n",
    "\n",
    "```python\n",
    "    # 年龄等于0的异常值进行剔除\n",
    "    data = data[data['age'] > 0]\n",
    "```\n",
    "\n",
    "对于变量NumberOfTime30-59DaysPastDueNotWorse、NumberOfTimes90DaysLate、NumberOfTime60-89DaysPastDueNotWorse这三个变量，由下面的箱线图图3-2可以看出，均存在异常值，且由unique函数可以得知均存在96、98两个异常值，因此予以剔除。同时会发现剔除其中一个变量的96、98值，其他变量的96、98两个值也会相应被剔除。\n",
    "\n",
    "![img](./img/7744536-fa32e3c1d7faa603.png)\n",
    "\n",
    "剔除变量NumberOfTime30-59DaysPastDueNotWorse、NumberOfTimes90DaysLate、NumberOfTime60-89DaysPastDueNotWorse的异常值。另外，数据集中好客户为0，违约客户为1，考虑到正常的理解，能正常履约并支付利息的客户为1，所以我们将其取反。\n",
    "\n",
    "```python\n",
    "    #剔除异常值\n",
    "    data = data[data['NumberOfTime30-59DaysPastDueNotWorse'] < 90]\n",
    "    #变量SeriousDlqin2yrs取反\n",
    "    data['SeriousDlqin2yrs']=1-data['SeriousDlqin2yrs']\n",
    "```\n",
    "\n",
    "### 数据切分\n",
    "\n",
    "为了验证模型的拟合效果，我们需要对数据集进行切分，分成训练集和测试集。\n",
    "\n",
    "```\n",
    "from sklearn.cross_validation import train_test_split\n",
    "```\n",
    "\n",
    "```python\n",
    "    Y = data['SeriousDlqin2yrs']\n",
    "    X = data.ix[:, 1:]\n",
    "    #测试集占比30%\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "    # print(Y_train)\n",
    "    train = pd.concat([Y_train, X_train], axis=1)\n",
    "    test = pd.concat([Y_test, X_test], axis=1)\n",
    "    clasTest = test.groupby('SeriousDlqin2yrs')['SeriousDlqin2yrs'].count()\n",
    "    train.to_csv('TrainData.csv',index=False)\n",
    "    test.to_csv('TestData.csv',index=False)\n",
    "```\n",
    "\n",
    "## 探索性分析\n",
    "\n",
    "在建立模型之前，我们一般会对现有的数据进行 探索性数据分析（Exploratory Data Analysis） 。 EDA是指对已有的数据(特别是调查或观察得来的原始数据)在尽量少的先验假定下进行探索。常用的探索性数据分析方法有：直方图、散点图和箱线图等。\n",
    "客户年龄分布如图4-1所示，可以看到年龄变量大致呈正态分布，符合统计分析的假设。\n",
    "\n",
    "![img](./img/7744536-deda01ff11dede30.png?)\n",
    "\n",
    "客户年收入分布如图4-2所示，月收入也大致呈正态分布，符合统计分析的需要。\n",
    "\n",
    "![img](./img/7744536-30a7f6a548713cd1.png)\n",
    "\n",
    "## 变量选择\n",
    "\n",
    "特征变量选择(排序)对于数据分析、机器学习从业者来说非常重要。好的特征选择能够提升模型的性能，更能帮助我们理解数据的特点、底层结构，这对进一步改善模型、算法都有着重要作用。至于Python的变量选择代码实现可以参考[结合Scikit-learn介绍几种常用的特征选择方法](https://www.cnblogs.com/hhh5460/p/5186226.html)。\n",
    "在本文中，我们采用信用评分模型的变量选择方法，通过WOE分析方法，即是通过比较指标分箱和对应分箱的违约概率来确定指标是否符合经济意义。首先我们对变量进行离散化（分箱）处理。\n",
    "\n",
    "### 分箱处理\n",
    "\n",
    "变量分箱（binning）是对连续变量离散化（discretization）的一种称呼。信用评分卡开发中一般有常用的等距分段、等深分段、最优分段。其中等距分段（Equval length intervals）是指分段的区间是一致的，比如年龄以十年作为一个分段；等深分段（Equal frequency intervals）是先确定分段数量，然后令每个分段中数据数量大致相等；最优分段（Optimal Binning）又叫监督离散化（supervised discretizaion），使用递归划分（Recursive Partitioning）将连续变量分为分段，背后是一种基于条件推断查找较佳分组的算法。\n",
    "我们首先选择对连续变量进行最优分段，在连续变量的分布不满足最优分段的要求时，再考虑对连续变量进行等距分段。最优分箱的代码如下：\n",
    "\n",
    "```python\n",
    "# 定义自动分箱函数\n",
    "def mono_bin(Y, X, n = 20):\n",
    "    r = 0\n",
    "    good=Y.sum()\n",
    "    bad=Y.count()-good\n",
    "    while np.abs(r) < 1:\n",
    "        d1 = pd.DataFrame({\"X\": X, \"Y\": Y, \"Bucket\": pd.qcut(X, n)})\n",
    "        d2 = d1.groupby('Bucket', as_index = True)\n",
    "        r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
    "        n = n - 1\n",
    "    d3 = pd.DataFrame(d2.X.min(), columns = ['min'])\n",
    "    d3['min']=d2.min().X\n",
    "    d3['max'] = d2.max().X\n",
    "    d3['sum'] = d2.sum().Y\n",
    "    d3['total'] = d2.count().Y\n",
    "    d3['rate'] = d2.mean().Y\n",
    "    d3['woe']=np.log((d3['rate']/(1-d3['rate']))/(good/bad))\n",
    "    d4 = (d3.sort_index(by = 'min')).reset_index(drop=True)\n",
    "    print(\"=\" * 60)\n",
    "    print(d4)\n",
    "    return d4\n",
    "```\n",
    "\n",
    "针对我们将使用最优分段对于数据集中的RevolvingUtilizationOfUnsecuredLines、age、DebtRatio和MonthlyIncome进行分类。\n",
    "\n",
    "![img](./img/7744536-48f4d9f97898e6b6.png)\n",
    "\n",
    "![img](./img/7744536-5d9d9713c9f4a238.png)\n",
    "\n",
    "![img](./img/7744536-6592fc15038c35a7.png)\n",
    "\n",
    "![img](./img/7744536-0ad338bd8aff769c.png)\n",
    "\n",
    "针对不能最优分箱的变量，分箱如下：\n",
    "\n",
    "```python\n",
    "    # 连续变量离散化\n",
    "    cutx3 = [ninf, 0, 1, 3, 5, pinf]\n",
    "    cutx6 = [ninf, 1, 2, 3, 5, pinf]\n",
    "    cutx7 = [ninf, 0, 1, 3, 5, pinf]\n",
    "    cutx8 = [ninf, 0,1,2, 3, pinf]\n",
    "    cutx9 = [ninf, 0, 1, 3, pinf]\n",
    "    cutx10 = [ninf, 0, 1, 2, 3, 5, pinf]\n",
    "```\n",
    "\n",
    "### WOE\n",
    "\n",
    "WoE分析， 是对指标分箱、计算各个档位的WoE值并观察WoE值随指标变化的趋势。其中WoE的数学定义是:\n",
    "woe=ln(goodattribute/badattribute)\n",
    "在进行分析时，我们需要对各指标从小到大排列，并计算出相应分档的WoE值。其中正向指标越大，WoE值越小；反向指标越大，WoE值越大。正向指标的WoE值负斜率越大，反响指标的正斜率越大，则说明指标区分能力好。WoE值趋近于直线，则意味指标判断能力较弱。若正向指标和WoE正相关趋势、反向指标同WoE出现负相关趋势，则说明此指标不符合经济意义，则应当予以去除。\n",
    "woe函数实现在上一节的mono_bin()函数里面已经包含，这里不再重复。\n",
    "\n",
    "### 相关性分析和IV筛选\n",
    "\n",
    "接下来，我们会用经过清洗后的数据看一下变量间的相关性。注意，这里的相关性分析只是初步的检查，进一步检查模型的VI（证据权重）作为变量筛选的依据。\n",
    "相关性图我们通过Python里面的seaborn包，调用heatmap()绘图函数进行绘制，实现代码如下：\n",
    "\n",
    "```python\n",
    "    corr = data.corr()#计算各变量的相关性系数\n",
    "    xticks = ['x0','x1','x2','x3','x4','x5','x6','x7','x8','x9','x10']#x轴标签\n",
    "    yticks = list(corr.index)#y轴标签\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(1, 1, 1)\n",
    "    sns.heatmap(corr, annot=True, cmap='rainbow', ax=ax1, annot_kws={'size': 9, 'weight': 'bold', 'color': 'blue'})#绘制相关性系数热力图\n",
    "    ax1.set_xticklabels(xticks, rotation=0, fontsize=10)\n",
    "    ax1.set_yticklabels(yticks, rotation=0, fontsize=10)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "生成的图形如图5-5所示：\n",
    "\n",
    "![img](./img/7744536-8c9384dae3f9d65d.png)\n",
    "\n",
    "数据集各变量的相关性\n",
    "\n",
    "由上图可以看出，各变量之间的相关性是非常小的。NumberOfOpenCreditLinesAndLoans和NumberRealEstateLoansOrLines的相关性系数为0.43。\n",
    "接下来，我进一步计算每个变量的Infomation Value（IV）。IV指标是一般用来确定自变量的预测能力。 其公式为：\n",
    "IV=sum((goodattribute-badattribute)\\*ln(goodattribute/badattribute))\n",
    "通过IV值判断变量预测能力的标准是：\n",
    "```\n",
    "< 0.02: unpredictive\n",
    "0.02 to 0.1: weak\n",
    "0.1 to 0.3: medium\n",
    "0.3 to 0.5: strong\n",
    "> 0.5: suspicious\n",
    "```\n",
    "IV的实现放在mono_bin()函数里面，代码实现如下：\n",
    "\n",
    "```python\n",
    "# 定义自动分箱函数\n",
    "def mono_bin(Y, X, n = 20):\n",
    "    r = 0\n",
    "    good=Y.sum()\n",
    "    bad=Y.count()-good\n",
    "    while np.abs(r) < 1:\n",
    "        d1 = pd.DataFrame({\"X\": X, \"Y\": Y, \"Bucket\": pd.qcut(X, n)})\n",
    "        d2 = d1.groupby('Bucket', as_index = True)\n",
    "        r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
    "        n = n - 1\n",
    "    d3 = pd.DataFrame(d2.X.min(), columns = ['min'])\n",
    "    d3['min']=d2.min().X\n",
    "    d3['max'] = d2.max().X\n",
    "    d3['sum'] = d2.sum().Y\n",
    "    d3['total'] = d2.count().Y\n",
    "    d3['rate'] = d2.mean().Y\n",
    "    d3['woe']=np.log((d3['rate']/(1-d3['rate']))/(good/bad))\n",
    "    d3['goodattribute']=d3['sum']/good\n",
    "    d3['badattribute']=(d3['total']-d3['sum'])/bad\n",
    "    iv=((d3['goodattribute']-d3['badattribute'])*d3['woe']).sum()\n",
    "    d4 = (d3.sort_index(by = 'min')).reset_index(drop=True)\n",
    "    print(\"=\" * 60)\n",
    "    print(d4)\n",
    "    cut=[]\n",
    "    cut.append(float('-inf'))\n",
    "    for i in range(1,n+1):\n",
    "        qua=X.quantile(i/(n+1))\n",
    "        cut.append(round(qua,4))\n",
    "    cut.append(float('inf'))\n",
    "    woe=list(d4['woe'].round(3))\n",
    "    return d4,iv,cut,woe\n",
    "```\n",
    "\n",
    "生成的IV图代码：\n",
    "\n",
    "```python\n",
    "    ivlist=[ivx1,ivx2,ivx3,ivx4,ivx5,ivx6,ivx7,ivx8,ivx9,ivx10]#各变量IV\n",
    "    index=['x1','x2','x3','x4','x5','x6','x7','x8','x9','x10']#x轴的标签\n",
    "    fig1 = plt.figure(1)\n",
    "    ax1 = fig1.add_subplot(1, 1, 1)\n",
    "    x = np.arange(len(index))+1\n",
    "    ax1.bar(x, ivlist, width=0.4)#生成柱状图\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(index, rotation=0, fontsize=12)\n",
    "    ax1.set_ylabel('IV(Information Value)', fontsize=14)\n",
    "    #在柱状图上添加数字标签\n",
    "    for a, b in zip(x, ivlist):\n",
    "        plt.text(a, b + 0.01, '%.4f' % b, ha='center', va='bottom', fontsize=10)\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "输出图像：\n",
    "\n",
    "![img](./img/7744536-18132fbec743bf20.png)\n",
    "\n",
    "输出的各变量IV图\n",
    "\n",
    "可以看出，DebtRatio、MonthlyIncome、NumberOfOpenCreditLinesAndLoans、NumberRealEstateLoansOrLines和NumberOfDependents变量的IV值明显较低，所以予以删除。\n",
    "\n",
    "## 小结\n",
    "以上介绍了信用评分模型开发过程中的数据预处理、探索性分析和变量选择。数据预处理主要针对缺失值用随机森林法和直接剔除法进行处理，对于异常值主要根据实际情况和箱形图的数据分布，对异常值进行剔除；探索性分析主要对各变量的分布情况进行初始的探究；变量选择主要考虑了变量的分箱方法，根据分箱结果计算WOE值，然后检查变量之间的相关性，根据各变量的IV值来选择对数据处理有好效果的变量。\n",
    "接下来会介绍信用评分模型的模型开发、模型评估和信用评分等。\n",
    "\n",
    "\n",
    "\n",
    "## 模型分析\n",
    "\n",
    "证据权重（Weight of Evidence,WOE）转换可以将Logistic回归模型转变为标准评分卡格式。引入WOE转换的目的并不是为了提高模型质量，只是一些变量不应该被纳入模型，这或者是因为它们不能增加模型值，或者是因为与其模型相关系数有关的误差较大，其实建立标准信用评分卡也可以不采用WOE转换。这种情况下，Logistic回归模型需要处理更大数量的自变量。尽管这样会增加建模程序的复杂性，但最终得到的评分卡都是一样的。\n",
    "在建立模型之前，我们需要将筛选后的变量转换为WoE值，便于信用评分。\n",
    "\n",
    "### WOE转换\n",
    "\n",
    "我们已经能获取了每个变量的分箱数据和woe数据，只需要根据各变量数据进行替换，实现代码如下：\n",
    "\n",
    "```python\n",
    "#替换成woe函数\n",
    "def replace_woe(series,cut,woe):\n",
    "    list=[]\n",
    "    i=0\n",
    "    while i<len(series):\n",
    "        value=series[i]\n",
    "        j=len(cut)-2\n",
    "        m=len(cut)-2\n",
    "        while j>=0:\n",
    "            if value>=cut[j]:\n",
    "                j=-1\n",
    "            else:\n",
    "                j -=1\n",
    "                m -= 1\n",
    "        list.append(woe[m])\n",
    "        i += 1\n",
    "    return list\n",
    "```\n",
    "\n",
    "我们将每个变量都进行替换,并将其保存到WoeData.csv文件中：\n",
    "\n",
    "```python\n",
    "    # 替换成woe\n",
    "    data['RevolvingUtilizationOfUnsecuredLines'] = Series(replace_woe(data['RevolvingUtilizationOfUnsecuredLines'], cutx1, woex1))\n",
    "    data['age'] = Series(replace_woe(data['age'], cutx2, woex2))\n",
    "    data['NumberOfTime30-59DaysPastDueNotWorse'] = Series(replace_woe(data['NumberOfTime30-59DaysPastDueNotWorse'], cutx3, woex3))\n",
    "    data['DebtRatio'] = Series(replace_woe(data['DebtRatio'], cutx4, woex4))\n",
    "    data['MonthlyIncome'] = Series(replace_woe(data['MonthlyIncome'], cutx5, woex5))\n",
    "    data['NumberOfOpenCreditLinesAndLoans'] = Series(replace_woe(data['NumberOfOpenCreditLinesAndLoans'], cutx6, woex6))\n",
    "    data['NumberOfTimes90DaysLate'] = Series(replace_woe(data['NumberOfTimes90DaysLate'], cutx7, woex7))\n",
    "    data['NumberRealEstateLoansOrLines'] = Series(replace_woe(data['NumberRealEstateLoansOrLines'], cutx8, woex8))\n",
    "    data['NumberOfTime60-89DaysPastDueNotWorse'] = Series(replace_woe(data['NumberOfTime60-89DaysPastDueNotWorse'], cutx9, woex9))\n",
    "    data['NumberOfDependents'] = Series(replace_woe(data['NumberOfDependents'], cutx10, woex10))\n",
    "    data.to_csv('WoeData.csv', index=False)\n",
    "```\n",
    "\n",
    "### Logisic Regression模型建立\n",
    "\n",
    "我们直接调用statsmodels包来实现逻辑回归：\n",
    "\n",
    "```python\n",
    "    导入数据\n",
    "    data = pd.read_csv('WoeData.csv')\n",
    "    #应变量\n",
    "    Y=data['SeriousDlqin2yrs']\n",
    "    #自变量，剔除对因变量影响不明显的变量\n",
    "    X=data.drop(['SeriousDlqin2yrs','DebtRatio','MonthlyIncome', 'NumberOfOpenCreditLinesAndLoans','NumberRealEstateLoansOrLines','NumberOfDependents'],axis=1)\n",
    "    X1=sm.add_constant(X)\n",
    "    logit=sm.Logit(Y,X1)\n",
    "    result=logit.fit()\n",
    "    print(result.summary())\n",
    "```\n",
    "\n",
    "输出结果：\n",
    "\n",
    "![img](./img/7744536-e7bb1738a73d563b.png)\n",
    "\n",
    "逻辑回归模型结果.png\n",
    "\n",
    "通过上图可知，逻辑回归各变量都已通过显著性检验，满足要求。\n",
    "\n",
    "### 模型检验\n",
    "\n",
    "到这里，我们的建模部分基本结束了。我们需要验证一下模型的预测能力如何。我们使用在建模开始阶段预留的test数据进行检验。通过ROC曲线和AUC来评估模型的拟合能力。\n",
    "在Python中，可以利用sklearn.metrics，它能方便比较两个分类器，自动计算ROC和AUC。\n",
    "实现代码：\n",
    "\n",
    "```python\n",
    "    #应变量\n",
    "    Y_test = test['SeriousDlqin2yrs']\n",
    "    #自变量，剔除对因变量影响不明显的变量，与模型变量对应\n",
    "    X_test = test.drop(['SeriousDlqin2yrs', 'DebtRatio', 'MonthlyIncome', 'NumberOfOpenCreditLinesAndLoans','NumberRealEstateLoansOrLines', 'NumberOfDependents'], axis=1)\n",
    "    X3 = sm.add_constant(X_test)\n",
    "    resu = result.predict(X3)#进行预测\n",
    "    fpr, tpr, threshold = roc_curve(Y_test, resu)\n",
    "    rocauc = auc(fpr, tpr)#计算AUC\n",
    "    plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % rocauc)#生成ROC曲线\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('真正率')\n",
    "    plt.xlabel('假正率')\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "输出结果：\n",
    "\n",
    "![img](./img/7744536-d805724cc8f99f9b.png)\n",
    "\n",
    "ROC曲线\n",
    "\n",
    "从上图可知，AUC值为0.85，说明该模型的预测效果还是不错的，正确率较高。\n",
    "\n",
    "## 信用评分\n",
    "\n",
    "我们已经基本完成了建模相关的工作，并用ROC曲线验证了模型的预测能力。接下来的步骤，就是将Logistic模型转换为标准评分卡的形式。\n",
    "\n",
    "### 评分标准\n",
    "\n",
    "![img](./img/7744536-cb7b3ab9620ddfe3.jpg)\n",
    "\n",
    "![img](./img/7744536-d966b6dadbffd4c1.jpg)\n",
    "\n",
    "a=log（p_good/P_bad）\n",
    "\n",
    "Score = offset + factor \\* log(odds)\n",
    "\n",
    "```python\n",
    "    # 我们取600分为基础分值，PDO为20（每高20分好坏比翻一倍），好坏比取20。\n",
    "    p = 20 / math.log(2)\n",
    "    q = 600 - 20 * math.log(20) / math.log(2)\n",
    "    baseScore = round(q + p * coe[0], 0)\n",
    "```\n",
    "\n",
    "**个人总评分=基础分+各部分得分**\n",
    "\n",
    "### 部分评分\n",
    "\n",
    "下面计算各变量部分的分数。各部分得分函数：\n",
    "\n",
    "```python\n",
    "#计算分数函数\n",
    "def get_score(coe,woe,factor):\n",
    "    scores=[]\n",
    "    for w in woe:\n",
    "        score=round(coe*w*factor,0)\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "```\n",
    "\n",
    "计算各变量得分情况：\n",
    "\n",
    "```python\n",
    "    # 各项部分分数\n",
    "    x1 = get_score(coe[1], woex1, p)\n",
    "    x2 = get_score(coe[2], woex2, p)\n",
    "    x3 = get_score(coe[3], woex3, p)\n",
    "    x7 = get_score(coe[4], woex7, p)\n",
    "    x9 = get_score(coe[5], woex9, p)\n",
    "```\n",
    "\n",
    "我们可以得到各部分的评分卡如图7-1所示：\n",
    "\n",
    "![img](./img/7744536-84440c1d99d6c276.png)\n",
    "\n",
    "各变量的评分标准\n",
    "\n",
    "## 自动评分系统\n",
    "\n",
    "根据变量来计算分数，实现如下：\n",
    "\n",
    "```python\n",
    "#根据变量计算分数\n",
    "def compute_score(series,cut,score):\n",
    "    list = []\n",
    "    i = 0\n",
    "    while i < len(series):\n",
    "        value = series[i]\n",
    "        j = len(cut) - 2\n",
    "        m = len(cut) - 2\n",
    "        while j >= 0:\n",
    "            if value >= cut[j]:\n",
    "                j = -1\n",
    "            else:\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "        list.append(score[m])\n",
    "        i += 1\n",
    "    return list\n",
    "```\n",
    "\n",
    "我们来计算test里面的分数：\n",
    "\n",
    "```python\n",
    "    test1 = pd.read_csv('TestData.csv')\n",
    "    test1['BaseScore']=Series(np.zeros(len(test1)))+baseScore\n",
    "    test1['x1'] = Series(compute_score(test1['RevolvingUtilizationOfUnsecuredLines'], cutx1, x1))\n",
    "    test1['x2'] = Series(compute_score(test1['age'], cutx2, x2))\n",
    "    test1['x3'] = Series(compute_score(test1['NumberOfTime30-59DaysPastDueNotWorse'], cutx3, x3))\n",
    "    test1['x7'] = Series(compute_score(test1['NumberOfTimes90DaysLate'], cutx7, x7))\n",
    "    test1['x9'] = Series(compute_score(test1['NumberOfTime60-89DaysPastDueNotWorse'], cutx9, x9))\n",
    "    test1['Score'] = test1['x1'] + test1['x2'] + test1['x3'] + test1['x7'] +test1['x9']  + baseScore\n",
    "    test1.to_csv('ScoreData.csv', index=False)\n",
    "```\n",
    "\n",
    "批量计算的部分分结果：\n",
    "\n",
    "![img](./img/7744536-3c1a735d1389c2b3.png)\n",
    "\n",
    "批量计算的部分结果\n",
    "\n",
    "## 总结以及展望\n",
    "\n",
    "这里通过对kaggle上的[Give Me Some Credit](https://link.jianshu.com?t=https://www.kaggle.com/c/GiveMeSomeCredit/data)数据的挖掘分析，结合信用评分卡的建立原理，从数据的预处理、变量选择、建模分析到创建信用评分，创建了一个简单的信用评分系统。\n",
    "基于AI 的机器学习评分卡系统可通过把旧数据（某个时间点后，例如2年）剔除掉后再进行自动建模、模型评估、并不断优化特征变量，使得系统更加强大。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
