{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/chinahadoop.png)\n",
    "# 互联网广告反欺诈建模-进阶集成解法之LightGBM\n",
    "**[小象学院](http://www.chinahadoop.cn/course/landpage/15)《机器学习集训营》课程资料 by [@寒小阳](http://www.chinahadoop.cn/user/49339/about)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入工具库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 功能函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据时减少内存使用的通用函数\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "traindata = pd.read_csv(\"data/round1_iflyad_anticheat_traindata.txt\",sep='\\t')\n",
    "testdata = pd.read_csv(\"data/round1_iflyad_anticheat_testdata_feature.txt\",sep='\\t')\n",
    "label = traindata.pop('label')\n",
    "test_id = testdata['sid'].values\n",
    "\n",
    "# 将训练集与测试集进行组合\n",
    "data=pd.concat([traindata,testdata],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理与基本特征构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sid最后有一个时间，把他提取出来，并且和请求服务到达时间做减法\n",
    "data['begin_time']=data['sid'].apply(lambda x:int(x.split('-')[-1])) ##请求会话时间\n",
    "data['nginxtime-begin_time']=data['nginxtime']-data['begin_time']##请求会话时间 与 请求到达服务时间的差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 针对nginxtime做特征工程\n",
    "data['date'] = pd.to_datetime(data['nginxtime'] / 1000, unit='s') + timedelta(hours=8)\n",
    "data['hour'] = data['date'].dt.hour.astype('int')\n",
    "data['minute'] = data['date'].dt.minute.astype('int')\n",
    "data['day'] = data['date'].dt.day.astype('int')#训练集是3到9天 测试集是第10天\n",
    "data['dayofweek'] = data['date'].dt.dayofweek.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将设备高和设备宽组合成设备的面积\n",
    "data['area'] = data['h'] * data['w']\n",
    "data['creative_dpi'] = data['w'].astype(str) + \"_\" + data['h'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idfamd5缺失严重，可以删除\n",
    "data.drop(['idfamd5'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把openudidmd5列按缺失与否转换成1和0\n",
    "data.loc[data['openudidmd5']=='empty', 'openudidmd5'] = 0\n",
    "data.loc[data['openudidmd5']!=0, 'openudidmd5'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 缺失值处理\n",
    "for col in ['city','lan','make','model','osv','ver']:\n",
    "    data[col].fillna('null_value', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orientation 出现异常值 90度和2 归为 0\n",
    "data.orientation[(data.orientation == 90) | (data.orientation == 2)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carrier  -1 就是0\n",
    "data.carrier[data.carrier == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os只有两个，Android和android,这两类与标签比例不同。转换为0和1，也可以亚编码\n",
    "data.loc[data['os']=='android', 'os'] = 0\n",
    "data.loc[data['os']=='Android', 'os'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运营商 carrier\n",
    "data.ntt[(data.carrier <= 0) | (data.carrier > 46003)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#osv\n",
    "def osv_fix(x):\n",
    "    if 'Android_' in x:\n",
    "        x = x.strip('Android_')\n",
    "    if 'Android' in x:\n",
    "        x = x.strip('Android')\n",
    "    return x\n",
    "\n",
    "data['osv'] = data['osv'].astype('str')\n",
    "data['osv'] = data['osv'].apply(osv_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make\n",
    "def make_fix(x):\n",
    "    x = x.lower()\n",
    "    if 'iphone' in x or 'apple' in x:\n",
    "        return 'apple'\n",
    "    if '华为' in x or 'huawei' in x or \"荣耀\" in x:\n",
    "        return 'huawei'\n",
    "    if \"魅族\" in x:\n",
    "        return 'meizu'\n",
    "    if \"金立\" in x:\n",
    "        return 'gionee'\n",
    "    if \"三星\" in x:\n",
    "        return 'samsung'\n",
    "    if 'xiaomi' in x or 'redmi' or '小米' in x:\n",
    "        return 'xiaomi'\n",
    "    if 'oppo' in x:\n",
    "        return 'oppo'\n",
    "    return x\n",
    "\n",
    "data['make'] = data['make'].astype('str').apply(lambda x: x.lower())\n",
    "data['make'] = data['make'].apply(make_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#删除无用的特征\n",
    "data.drop(['sid','nginxtime','begin_time'],axis=1, inplace=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pkgname\n",
      "ver\n",
      "adunitshowid\n",
      "mediashowid\n",
      "ip\n",
      "city\n",
      "province\n",
      "reqrealip\n",
      "adidmd5\n",
      "imeimd5\n",
      "macmd5\n",
      "model\n",
      "make\n",
      "osv\n",
      "lan\n",
      "creative_dpi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#编码，加速\n",
    "categorical_features = ['pkgname','ver','adunitshowid','mediashowid','ip','city','province',\n",
    "    'reqrealip','adidmd5','imeimd5','macmd5','model','make','osv','lan','creative_dpi']\n",
    "\n",
    "for col in categorical_features:\n",
    "    print(col)\n",
    "    data[col] = data[col].map(\n",
    "        dict(zip(data[col].unique(), range(0, data[col].nunique()))))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聚合特征和统计特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pkgname\n",
      "ver\n",
      "adunitshowid\n",
      "mediashowid\n",
      "apptype\n",
      "ip\n",
      "city\n",
      "province\n",
      "reqrealip\n",
      "adidmd5\n",
      "imeimd5\n",
      "macmd5\n",
      "dvctype\n",
      "model\n",
      "make\n",
      "ntt\n",
      "carrier\n",
      "osv\n",
      "creative_dpi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_cate_feature = ['pkgname','ver','adunitshowid','mediashowid','apptype']   \n",
    "ip_cate_feature = ['ip','city','province','reqrealip'] \n",
    "model_cate_feature = ['adidmd5', 'imeimd5',  'macmd5','dvctype','model','make','ntt','carrier','osv','creative_dpi']\n",
    "\n",
    "#将一些特征的出现次数作为特征,命名为vc_+特证名\n",
    "value_counts_col = media_cate_feature+ip_cate_feature+model_cate_feature\n",
    "count_feature_list = []\n",
    "def feature_count(data, features=[], is_feature=True):\n",
    "    if len(set(features)) != len(features):\n",
    "        print('equal feature !!!!')\n",
    "        return data\n",
    "    new_feature = 'count'\n",
    "    nunique = []\n",
    "    for i in features:\n",
    "        nunique.append(data[i].nunique())\n",
    "        new_feature += '_' + i.replace('add_', '')\n",
    "    if len(features) > 1 and len(data[features].drop_duplicates()) <= np.max(nunique):\n",
    "        print(new_feature, 'is unvalid cross feature:')\n",
    "        return data\n",
    "    temp = data.groupby(features).size().reset_index().rename(columns={0: new_feature})\n",
    "    data = data.merge(temp, 'left', on=features)\n",
    "    if is_feature:\n",
    "        count_feature_list.append(new_feature)\n",
    "    return data\n",
    "\n",
    "\n",
    "for i in value_counts_col:\n",
    "    print(i)\n",
    "    n = data[i].nunique()\n",
    "    if n > 5:\n",
    "        data = feature_count(data, [i])\n",
    "        data = feature_count(data, ['day', 'hour', i])    \n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city\n",
      "model\n",
      "reqrealip\n",
      "ip\n",
      "ver\n",
      "osv\n",
      "creative_dpi\n"
     ]
    }
   ],
   "source": [
    "ratio_feature_list = []\n",
    "for i in ['adunitshowid']:\n",
    "    for j in ['city','model','reqrealip','ip','ver','osv','creative_dpi']:\n",
    "        print(j)\n",
    "        data = feature_count(data, [i, j])\n",
    "        if data[i].nunique() > 5 and data[j].nunique() > 5:\n",
    "            data['ratio_' + j + '_of_' + i] = data['count_' + i + '_' + j] / data['count_' + i]\n",
    "            data['ratio_' + i + '_of_' + j] = data['count_' + i + '_' + j] / data['count_' + j]\n",
    "            ratio_feature_list.append('ratio_' + j + '_of_' + i)\n",
    "            ratio_feature_list.append('ratio_' + i + '_of_' + j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pkgname\n",
      "ver\n",
      "adunitshowid\n",
      "mediashowid\n",
      "apptype\n",
      "ip\n",
      "city\n",
      "province\n",
      "reqrealip\n",
      "adidmd5\n",
      "imeimd5\n",
      "macmd5\n",
      "model\n",
      "make\n",
      "ntt\n",
      "osv\n",
      "creative_dpi\n"
     ]
    }
   ],
   "source": [
    "# 低频过滤\n",
    "for feature in value_counts_col:\n",
    "    if 'count_' + feature in data.keys():\n",
    "        print(feature)\n",
    "        data.loc[data['count_' + feature] < 2, feature] = -1\n",
    "        data[feature] = data[feature] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(info):\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) + ' ' + str(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-31 17:07:48 Cal next_time_delta\n",
      "2020-03-31 17:07:48 Calculate next_time_delta...\n",
      "2020-03-31 17:07:48 Calculate next_time_delta...\n",
      "2020-03-31 17:07:48 Calculate next_time_delta...\n",
      "2020-03-31 17:07:48 Calculate next_time_delta...\n",
      "2020-03-31 17:07:49 Calculate next_time_delta...\n",
      "2020-03-31 17:07:49 Calculate next_time_delta...\n",
      "2020-03-31 17:07:49 Calculate next_time_delta...\n",
      "2020-03-31 17:07:49 Calculate next_time_delta...\n",
      "2020-03-31 17:07:49 Calculate next_time_delta...\n",
      "2020-03-31 17:07:49 Calculate next_time_delta...\n",
      "2020-03-31 17:07:49 Calculate next_time_delta...\n"
     ]
    }
   ],
   "source": [
    "predictors = []    \n",
    "'''\n",
    "ip = ip\n",
    "app = adunitshowid\n",
    "device = adidmd5\n",
    "os = model\n",
    "channel = mediashowid\n",
    "click_time = nginxtime\n",
    "date \n",
    "'''\n",
    "#计算之后的时间差    \n",
    "def cal_next_time_delta(df, suffix, type='float32'):\n",
    "    groupby_columns = [\n",
    "        {'columns': ['model']},\n",
    "       {'columns': ['city']},\n",
    "        {'columns': ['adunitshowid']},\n",
    "       {'columns': ['reqrealip']},\n",
    "       {'columns': ['creative_dpi']},\n",
    "       {'columns': ['adunitshowid']},\n",
    "       {'columns': ['ip']},\n",
    "       {'columns': ['model','adunitshowid']},\n",
    "       {'columns': ['model','creative_dpi']},\n",
    "       {'columns': ['mediashowid']},\n",
    "       {'columns': ['model', 'city']},\n",
    "    ]\n",
    "    # Calculate the time to next click for each group\n",
    "    for spec in groupby_columns:\n",
    "        # Name of new feature\n",
    "        new_name = '{}_{}'.format('_'.join(spec['columns']), suffix)\n",
    "        # Unique list of features to select\n",
    "        all_features = spec['columns'] + ['date']\n",
    "        # Run calculation\n",
    "        log('Calculate ' + suffix + '...')\n",
    "        df[new_name] = (df[all_features].groupby(spec['columns']).date.shift(-1) - df.date).dt.seconds.astype(type)\n",
    "        predictors.append(new_name)\n",
    "        gc.collect()\n",
    "    return df    \n",
    "    \n",
    "log('Cal next_time_delta')\n",
    "data = cal_next_time_delta(data, 'next_time_delta', 'float32')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-31 17:07:55 Cal prev_time_delta\n",
      "2020-03-31 17:07:55 Calculate prev_time_delta...\n",
      "2020-03-31 17:07:55 Calculate prev_time_delta...\n",
      "2020-03-31 17:07:55 Calculate prev_time_delta...\n",
      "2020-03-31 17:07:55 Calculate prev_time_delta...\n",
      "2020-03-31 17:07:55 Calculate prev_time_delta...\n",
      "2020-03-31 17:07:56 Calculate prev_time_delta...\n",
      "2020-03-31 17:07:56 Calculate prev_time_delta...\n",
      "2020-03-31 17:07:56 Calculate prev_time_delta...\n",
      "2020-03-31 17:07:56 Calculate prev_time_delta...\n",
      "2020-03-31 17:07:56 Calculate prev_time_delta...\n",
      "2020-03-31 17:07:56 Calculate prev_time_delta...\n"
     ]
    }
   ],
   "source": [
    "#计算之前的时间差    \n",
    "def cal_prev_time_delta(df, suffix, type='float32'):\n",
    "    groupby_columns = [\n",
    "        {'columns': ['model']},\n",
    "       {'columns': ['city']},\n",
    "        {'columns': ['adunitshowid']},\n",
    "       {'columns': ['reqrealip']},\n",
    "       {'columns': ['creative_dpi']},\n",
    "       {'columns': ['adunitshowid']},\n",
    "       {'columns': ['ip']},\n",
    "       {'columns': ['model','adunitshowid']},\n",
    "       {'columns': ['model','creative_dpi']},\n",
    "       {'columns': ['mediashowid']},\n",
    "       {'columns': ['model', 'city']},\n",
    "    ]\n",
    "    # Calculate the time to prev click for each group\n",
    "    for spec in groupby_columns:\n",
    "        # Name of new feature\n",
    "        new_name = '{}_{}'.format('_'.join(spec['columns']), suffix)\n",
    "        # Unique list of features to select\n",
    "        all_features = spec['columns'] + ['date']\n",
    "        # Run calculation\n",
    "        log('Calculate ' + suffix + '...')\n",
    "        df[new_name] = (df.date - df[all_features].groupby(spec['columns']).date.shift(+1)).dt.seconds.astype(type)\n",
    "        predictors.append(new_name)\n",
    "        gc.collect()\n",
    "    return df    \n",
    "    \n",
    "log('Cal prev_time_delta')\n",
    "data = cal_prev_time_delta(data, 'prev_time_delta', 'float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除无用的特征\n",
    "data.drop(['date'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-31 17:07:57 Cal nunique_model_gb_ip\n",
      "2020-03-31 17:08:00 Cal nunique_adunitshowid_gb_ip\n",
      "2020-03-31 17:08:04 Cal nunique_city_gb_model\n",
      "2020-03-31 17:08:07 Cal nunique_reqrealip_gb_model\n",
      "2020-03-31 17:08:11 Cal nunique_adunitshowid_gb_model\n",
      "2020-03-31 17:08:14 Cal nunique_model_gb_adunitshowid\n",
      "2020-03-31 17:08:18 Cal nunique_model_gb_reqrealip\n",
      "2020-03-31 17:08:21 Cal nunique_reqrealip_gb_adunitshowid\n",
      "2020-03-31 17:08:25 Cal nunique_ip_gb_reqrealip\n",
      "2020-03-31 17:08:28 Cal nunique_hour_gb_model_day\n",
      "2020-03-31 17:08:32 Cal nunique_hour_gb_city_day\n",
      "2020-03-31 17:08:36 Cal nunique_day_gb_ip_adunitshowid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算唯一值特征    \n",
    "def merge_nunique(df, columns_groupby, column, new_column_name, type='uint64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].nunique()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df    \n",
    "\n",
    "log('Cal nunique_model_gb_ip')\n",
    "data = merge_nunique(data, ['ip'], 'model', 'nunique_model_gb_ip', 'uint32')\n",
    "gc.collect()    \n",
    "    \n",
    "log('Cal nunique_adunitshowid_gb_ip')\n",
    "data = merge_nunique(data, ['ip'], 'adunitshowid', 'nunique_adunitshowid_gb_ip', 'uint32')\n",
    "gc.collect() \n",
    "\n",
    "log('Cal nunique_city_gb_model')\n",
    "data = merge_nunique(data, ['model'], 'city', 'nunique_city_gb_model', 'uint32')\n",
    "gc.collect()     \n",
    "\n",
    "log('Cal nunique_reqrealip_gb_model')\n",
    "data = merge_nunique(data, ['model'], 'reqrealip', 'nunique_reqrealip_gb_model', 'uint32')\n",
    "gc.collect()  \n",
    "\n",
    "log('Cal nunique_adunitshowid_gb_model')\n",
    "data = merge_nunique(data, ['model'], 'adunitshowid', 'nunique_adunitshowid_gb_model', 'uint32')\n",
    "gc.collect()  \n",
    "\n",
    "log('Cal nunique_model_gb_adunitshowid')\n",
    "data = merge_nunique(data, ['adunitshowid'], 'model', 'nunique_model_gb_adunitshowid', 'uint32')\n",
    "gc.collect()   \n",
    "\n",
    "log('Cal nunique_model_gb_reqrealip')\n",
    "data = merge_nunique(data, ['reqrealip'], 'model', 'nunique_model_gb_reqrealip', 'uint32')\n",
    "gc.collect()    \n",
    "    \n",
    "log('Cal nunique_reqrealip_gb_adunitshowid')\n",
    "data = merge_nunique(data, ['adunitshowid'], 'reqrealip', 'nunique_reqrealip_gb_adunitshowid', 'uint32')\n",
    "gc.collect()   \n",
    "\n",
    "log('Cal nunique_ip_gb_reqrealip')\n",
    "data = merge_nunique(data, ['reqrealip'], 'ip', 'nunique_ip_gb_reqrealip', 'uint32')\n",
    "gc.collect()  \n",
    "\n",
    "log('Cal nunique_hour_gb_model_day')\n",
    "data = merge_nunique(data, ['model', 'day'], 'hour', 'nunique_hour_gb_model_day', 'uint32')\n",
    "gc.collect()\n",
    "\n",
    "log('Cal nunique_hour_gb_city_day')\n",
    "data = merge_nunique(data, ['city', 'day'], 'hour', 'nunique_hour_gb_city_day', 'uint32')\n",
    "gc.collect()\n",
    "\n",
    "log('Cal nunique_day_gb_ip_adunitshowid')\n",
    "data = merge_nunique(data, ['reqrealip', 'day'], 'hour', 'nunique_day_gb_reqrealip_day', 'uint32')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-31 17:08:39 Cal cumcount_city_gb_model\n",
      "2020-03-31 17:08:40 Cal cumcount_reqrealip_gb_model\n",
      "2020-03-31 17:08:40 Cal cumcount_adunitshowid_gb_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算cumcount特征\n",
    "def merge_cumcount(df, columns_groupby, column, new_column_name, type='uint64'):\n",
    "    df[new_column_name] = df.groupby(columns_groupby)[column].cumcount().values.astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "log('Cal cumcount_city_gb_model')\n",
    "data = merge_cumcount(data, ['model'], 'city', 'cumcount_city_gb_model', 'uint32');\n",
    "gc.collect()\n",
    "\n",
    "log('Cal cumcount_reqrealip_gb_model')\n",
    "data = merge_cumcount(data, ['model'], 'reqrealip', 'cumcount_reqrealip_gb_model', 'uint32');\n",
    "gc.collect()\n",
    "\n",
    "log('Cal cumcount_adunitshowid_gb_model')\n",
    "data = merge_cumcount(data, ['model'], 'adunitshowid', 'cumcount_adunitshowid_gb_model', 'uint32');\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-31 17:08:40 Cal count_gb_model_city\n",
      "2020-03-31 17:08:43 Cal count_gb_model_reqrealip\n",
      "2020-03-31 17:08:47 Cal count_gb_model_adunitshowid\n",
      "2020-03-31 17:08:50 Cal count_gb_model_ip\n",
      "2020-03-31 17:08:54 Cal count_gb_city_reqrealip\n",
      "2020-03-31 17:08:58 Cal count_gb_city_adunitshowid\n",
      "2020-03-31 17:09:01 Cal count_gb_reqrealip_adunitshowid\n",
      "2020-03-31 17:09:04 Cal count_gb_reqrealip_ip\n",
      "2020-03-31 17:09:08 Cal count_gb_city_hour\n",
      "2020-03-31 17:09:11 Cal count_gb_model_hour\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算count特征\n",
    "def merge_count(df, columns_groupby, new_column_name, type='uint64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby).size()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "log('Cal count_gb_model_city')\n",
    "data = merge_count(data, ['model', 'city'], 'count_gb_model_city', 'uint32');\n",
    "gc.collect()\n",
    "\n",
    "log('Cal count_gb_model_reqrealip')\n",
    "data = merge_count(data, ['model', 'reqrealip'], 'count_gb_model_reqrealip', 'uint32');\n",
    "gc.collect()\n",
    "\n",
    "log('Cal count_gb_model_adunitshowid')\n",
    "data = merge_count(data, ['model', 'adunitshowid'], 'count_gb_model_adunitshowid', 'uint32');\n",
    "gc.collect()\n",
    "\n",
    "log('Cal count_gb_model_ip')\n",
    "data = merge_count(data, ['model', 'ip'], 'count_gb_model_ip', 'uint32');\n",
    "gc.collect()\n",
    "\n",
    "log('Cal count_gb_city_reqrealip')\n",
    "data = merge_count(data, ['city', 'reqrealip'], 'count_gb_city_reqrealip', 'uint32');\n",
    "gc.collect()\n",
    "\n",
    "log('Cal count_gb_city_adunitshowid')\n",
    "data = merge_count(data, ['city', 'adunitshowid'], 'count_gb_city_adunitshowid', 'uint32');\n",
    "gc.collect()\n",
    "\n",
    "log('Cal count_gb_reqrealip_adunitshowid')\n",
    "data = merge_count(data, ['reqrealip', 'adunitshowid'], 'count_gb_reqrealip_adunitshowid', 'uint32');\n",
    "gc.collect()\n",
    "\n",
    "log('Cal count_gb_reqrealip_ip')\n",
    "data = merge_count(data, ['reqrealip', 'ip'], 'count_gb_reqrealip_ip', 'uint32');\n",
    "gc.collect()\n",
    "\n",
    "log('Cal count_gb_city_hour')\n",
    "data = merge_count(data, ['city', 'hour'], 'count_gb_city_hour', 'uint32');\n",
    "gc.collect()\n",
    "\n",
    "log('Cal count_gb_model_hour')\n",
    "data = merge_count(data, ['model', 'hour'], 'count_gb_model_hour', 'uint32');\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-31 17:09:15 Cal var_day_gb_model\n",
      "2020-03-31 17:09:18 Cal var_hour_gb_model\n",
      "2020-03-31 17:09:22 Cal var_day_gb_city\n",
      "2020-03-31 17:09:25 Cal var_hour_gb_city\n",
      "2020-03-31 17:09:28 Cal var_day_gb_reqrealip\n",
      "2020-03-31 17:09:32 Cal var_hour_gb_reqrealip\n",
      "2020-03-31 17:09:35 Cal var_day_gb_adunitshowid\n",
      "2020-03-31 17:09:39 Cal var_hour_gb_mediashowid\n"
     ]
    }
   ],
   "source": [
    "#计算方差特征\n",
    "def merge_var(df, columns_groupby, column, new_column_name, type='float64'):\n",
    "    add = pd.DataFrame(df.groupby(columns_groupby)[column].var()).reset_index()\n",
    "    add.columns = columns_groupby + [new_column_name]\n",
    "    df = df.merge(add, on=columns_groupby, how=\"left\")\n",
    "    df[new_column_name] = df[new_column_name].astype(type)\n",
    "    predictors.append(new_column_name)\n",
    "    return df\n",
    "\n",
    "log('Cal var_day_gb_model')\n",
    "data = merge_var(data, ['model'], 'day', 'var_day_gb_model', 'float32')\n",
    "gc.collect()\n",
    "\n",
    "log('Cal var_hour_gb_model')\n",
    "data = merge_var(data, ['model'], 'hour', 'var_hour_gb_model', 'float32')\n",
    "gc.collect()\n",
    "\n",
    "log('Cal var_day_gb_city')\n",
    "data = merge_var(data, ['city'], 'day', 'var_day_gb_city', 'float32')\n",
    "gc.collect()\n",
    "\n",
    "log('Cal var_hour_gb_city')\n",
    "data = merge_var(data, ['city'], 'hour', 'var_hour_gb_city', 'float32')\n",
    "gc.collect()\n",
    "\n",
    "log('Cal var_day_gb_reqrealip')\n",
    "data = merge_var(data, ['reqrealip'], 'day', 'var_day_gb_reqrealip', 'float32')\n",
    "gc.collect()\n",
    "\n",
    "log('Cal var_hour_gb_reqrealip')\n",
    "data = merge_var(data, ['reqrealip'], 'hour', 'var_hour_gb_reqrealip', 'float32')\n",
    "gc.collect()\n",
    "\n",
    "log('Cal var_day_gb_adunitshowid')\n",
    "data = merge_var(data, ['adunitshowid'], 'day', 'var_day_gb_adunitshowid', 'float32')\n",
    "gc.collect()\n",
    "\n",
    "log('Cal var_hour_gb_mediashowid')\n",
    "data = merge_var(data, ['mediashowid'], 'hour', 'var_hour_gb_mediashowid', 'float32')\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "data.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 429.06 Mb (55.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "#减少内存的使用\n",
    "data = reduce_mem_usage(data)\n",
    "\n",
    "\n",
    "cat_feature = value_counts_col+['openudidmd5','os','orientation','lan']\n",
    "num_feature = [col for col in data.columns if col not in cat_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立countVector特征\n",
    "data['new_con'] = data['model'].astype(str)\n",
    "for i in ['city', 'reqrealip','adunitshowid','mediashowid']:\n",
    "    data['new_con'] = data['new_con'].astype(str) + '_' + data[i].astype(str)\n",
    "data['new_con'] = data['new_con'].apply(lambda x: ' '.join(x.split('_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot prepared !\n",
      "cv prepared !\n"
     ]
    }
   ],
   "source": [
    "##划分数据：\n",
    "train=data[:traindata.shape[0]]\n",
    "test=data[traindata.shape[0]:]\n",
    "predict_result = pd.DataFrame()\n",
    "predict_result['sid'] = test_id\n",
    "predict_result['label'] = 0\n",
    "train_y = label.values\n",
    "\n",
    "base_train_csr = sparse.csr_matrix((len(train), 0))\n",
    "base_predict_csr = sparse.csr_matrix((len(test), 0))\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "for feature in cat_feature:\n",
    "    enc.fit(data[feature].values.reshape(-1, 1))\n",
    "    base_train_csr = sparse.hstack(\n",
    "    (base_train_csr, enc.transform(train[feature].values.reshape(-1, 1))), 'csr','bool')\n",
    "    base_predict_csr = sparse.hstack(\n",
    "    (base_predict_csr, enc.transform(test[feature].values.reshape(-1, 1))),'csr','bool')\n",
    "print('one-hot prepared !')\n",
    "\n",
    "\n",
    "cv = CountVectorizer(min_df=10)\n",
    "for feature in ['new_con']:\n",
    "    data[feature] = data[feature].astype(str)\n",
    "    cv.fit(data[feature])\n",
    "    base_train_csr = sparse.hstack((base_train_csr, cv.transform(train[feature].astype(str))), 'csr', 'bool')\n",
    "    base_predict_csr = sparse.hstack((base_predict_csr, cv.transform(test[feature].astype(str))), 'csr',\n",
    "                                     'bool')\n",
    "print('cv prepared !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csr = sparse.hstack(\n",
    "    (sparse.csr_matrix(train[num_feature]), base_train_csr), 'csr').astype(\n",
    "    'float32')\n",
    "predict_csr = sparse.hstack(\n",
    "    (sparse.csr_matrix(test[num_feature]), base_predict_csr), 'csr').astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建模、调优与预估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[10]\tvalid_0's binary_logloss: 0.340197\tvalid_0's f1: 0.910181\tvalid_1's binary_logloss: 0.339832\tvalid_1's f1: 0.910259\n",
      "[20]\tvalid_0's binary_logloss: 0.243891\tvalid_0's f1: 0.916618\tvalid_1's binary_logloss: 0.243271\tvalid_1's f1: 0.916606\n",
      "[30]\tvalid_0's binary_logloss: 0.206159\tvalid_0's f1: 0.922124\tvalid_1's binary_logloss: 0.205527\tvalid_1's f1: 0.921994\n",
      "[40]\tvalid_0's binary_logloss: 0.187917\tvalid_0's f1: 0.925664\tvalid_1's binary_logloss: 0.187469\tvalid_1's f1: 0.925382\n",
      "[50]\tvalid_0's binary_logloss: 0.178356\tvalid_0's f1: 0.928088\tvalid_1's binary_logloss: 0.178274\tvalid_1's f1: 0.927882\n",
      "[60]\tvalid_0's binary_logloss: 0.17201\tvalid_0's f1: 0.930076\tvalid_1's binary_logloss: 0.172218\tvalid_1's f1: 0.929669\n",
      "[70]\tvalid_0's binary_logloss: 0.168046\tvalid_0's f1: 0.931349\tvalid_1's binary_logloss: 0.168702\tvalid_1's f1: 0.9305\n",
      "[80]\tvalid_0's binary_logloss: 0.164835\tvalid_0's f1: 0.932422\tvalid_1's binary_logloss: 0.165877\tvalid_1's f1: 0.931663\n",
      "[90]\tvalid_0's binary_logloss: 0.162458\tvalid_0's f1: 0.933186\tvalid_1's binary_logloss: 0.164044\tvalid_1's f1: 0.932379\n",
      "[100]\tvalid_0's binary_logloss: 0.160437\tvalid_0's f1: 0.933897\tvalid_1's binary_logloss: 0.162613\tvalid_1's f1: 0.932828\n",
      "[110]\tvalid_0's binary_logloss: 0.158915\tvalid_0's f1: 0.934608\tvalid_1's binary_logloss: 0.161817\tvalid_1's f1: 0.933235\n",
      "[120]\tvalid_0's binary_logloss: 0.157486\tvalid_0's f1: 0.935183\tvalid_1's binary_logloss: 0.16115\tvalid_1's f1: 0.933507\n",
      "[130]\tvalid_0's binary_logloss: 0.156102\tvalid_0's f1: 0.935717\tvalid_1's binary_logloss: 0.160558\tvalid_1's f1: 0.933579\n",
      "[140]\tvalid_0's binary_logloss: 0.154837\tvalid_0's f1: 0.936182\tvalid_1's binary_logloss: 0.160009\tvalid_1's f1: 0.933853\n",
      "[150]\tvalid_0's binary_logloss: 0.153717\tvalid_0's f1: 0.936587\tvalid_1's binary_logloss: 0.15976\tvalid_1's f1: 0.933883\n",
      "[160]\tvalid_0's binary_logloss: 0.152569\tvalid_0's f1: 0.937044\tvalid_1's binary_logloss: 0.15939\tvalid_1's f1: 0.934046\n",
      "[170]\tvalid_0's binary_logloss: 0.151509\tvalid_0's f1: 0.937647\tvalid_1's binary_logloss: 0.159079\tvalid_1's f1: 0.934002\n"
     ]
    }
   ],
   "source": [
    "def lgb_f1(labels, preds):\n",
    "    score = f1_score(labels, np.round(preds))\n",
    "    return 'f1', score, True\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(random_seed=2019, n_jobs=-1, objective='binary',\n",
    "                     learning_rate=0.1, n_estimators=500, num_leaves=100, max_depth=-1,\n",
    "                     min_child_samples=20, min_child_weight=9, subsample_freq=1,\n",
    "                     subsample=0.8, colsample_bytree=0.8, reg_alpha=1, reg_lambda=5)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# lgb_model = lgb.LGBMClassifier(\n",
    "#     boosting_type='gbdt', num_leaves=61, reg_alpha=3, reg_lambda=1,\n",
    "#     max_depth=-1, n_estimators=5000, objective='binary',\n",
    "#     subsample=0.8, colsample_bytree=0.8, subsample_freq=1,\n",
    "#     learning_rate=0.035, random_state=2018, n_jobs=10\n",
    "# )\n",
    "# =============================================================================\n",
    "skf = StratifiedKFold(n_splits=5, random_state=2019, shuffle=True)\n",
    "best_score = []\n",
    "for index, (train_index, test_index) in enumerate(skf.split(train_csr, train_y)):\n",
    "    lgb_model.fit(train_csr[train_index], train_y[train_index],\n",
    "                  eval_set=[(train_csr[train_index], train_y[train_index]),\n",
    "                            (train_csr[test_index], train_y[test_index])],\n",
    "                            eval_metric=lgb_f1,early_stopping_rounds=200, verbose=10)\n",
    "    best_score.append(lgb_model.best_score_['valid_1']['binary_logloss'])\n",
    "    print(best_score)\n",
    "    test_pred = lgb_model.predict_proba(predict_csr, num_iteration=lgb_model.best_iteration_)[:, 1]\n",
    "    predict_result['label'] = predict_result['label'] + test_pred\n",
    "predict_result['label'] = predict_result['label'] / 5\n",
    "mean = predict_result['label'].mean()\n",
    "predict_result['label']=predict_result['label'].apply(lambda x: 1 if x>=0.5 else 0) ##∪概率大于0.5的置1，否则置0\n",
    "print('test pre_label distribution:\\n',predict_result['label'].value_counts()) ## 模型预测测试集的标签分布\n",
    "predict_result.to_csv('submit081202.csv',index=None) ##保存为submit0704.csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.values\n",
    "y = label.values\n",
    "X_test = test.values\n",
    "\n",
    "# =============================================================================\n",
    "# def featureSelect(x_train,y_train,x_val,func=chi2,percentile=80):\n",
    "#     model = SelectPercentile(func,percentile=percentile)\n",
    "#     model.fit(x_train,y_train)\n",
    "#     x_train = model.transform(x_train)\n",
    "#     x_val = model.transform(x_val)\n",
    "#     return x_train,x_val\n",
    "# X,X_test = featureSelect(X,y,X_test)\n",
    "# \n",
    "# =============================================================================\n",
    "\n",
    "##模型训练预测：\n",
    "#oof_lgb,prediction_lgb,feature_importance_df = lgb_model(train, label, test)\n",
    "\n",
    "def f1_score_vail(pred, data_vail):\n",
    "    labels = data_vail.get_label()\n",
    "   # pred = np.argmax(pred.reshape(2, -1), axis=0)      # lgb的predict输出为各类型概率值\n",
    "    temp = pd.DataFrame()\n",
    "    temp['pred'] = pred\n",
    "    temp['pred'] = temp['pred'].apply(lambda x: 1 if x>=0.5 else 0)\n",
    "    score_vail = f1_score(y_true=labels, y_pred=temp['pred'].values, average='macro')\n",
    "    return 'f1_score', score_vail, True\n",
    "\n",
    "\n",
    "\n",
    "lgb_params = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"lambda_l1\": 0.1,\n",
    "    \"lambda_l2\": 0.2,\n",
    "    \"max_depth\": -1,\n",
    "    \"num_leaves\": 120,\n",
    "    \"objective\": \"binary\",\n",
    "    \"verbose\": -1,\n",
    "    'feature_fraction': 0.8,\n",
    "    \"min_split_gain\": 0.1,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"subsample\": 0.8,\n",
    "    \"min_data_in_leaf\": 50,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"colsample_bylevel\": 0.7,\n",
    "    \"tree_method\": 'exact',\n",
    "   # \"seed\":2019\n",
    "}\n",
    "\n",
    "skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "oof_lgb=np.zeros(train.shape[0]) ##用于存放训练集概率，由每折验证集所得\n",
    "prediction_lgb=np.zeros(test.shape[0])  ##用于存放测试集概率，k折最后要除以k取平均\n",
    "feature_importance_df = pd.DataFrame() ##存放特征重要性，此处不考虑\n",
    "for index,(train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print('fold:',index+1,'training')\n",
    "    X_train, X_valid, y_train, y_valid = X[train_index], X[test_index], y[train_index], y[test_index] \n",
    "    train_data = lgb.Dataset(X_train, label=y_train,)\n",
    "    #feature_name=train.columns.tolist(), categorical_feature=categorical_features)    # 训练数据\n",
    "    validation_data = lgb.Dataset(X_valid, label=y_valid,)\n",
    "    #feature_name=train.columns.tolist(), categorical_feature=categorical_features)   # 验证数据\n",
    "    ##训练：\n",
    "    clf = lgb.train(lgb_params, train_data, num_boost_round=1500, valid_sets=[validation_data],\n",
    "                   # categorical_feature=list(range(0, 15)),\n",
    "                    early_stopping_rounds=100, feval=f1_score_vail, verbose_eval=100)     # 训练\n",
    "    \n",
    "    ##预测验证集：\n",
    "    oof_lgb[test_index] += clf.predict(X_valid, num_iteration=clf.best_iteration)\n",
    "    ##预测测试集：\n",
    "    prediction_lgb += clf.predict(X_test, num_iteration=clf.best_iteration)  # 预测\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = list(train.columns)\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = index + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "\n",
    "print('the roc_auc_score for train:',f1_score(y,np.round(oof_lgb))) ##线下auc评分0.9340750799421788"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_lgb/=5\n",
    "feature_importance = feature_importance_df.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "\n",
    "feature_importance.to_csv('fea_importance.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##保存结果：\n",
    "sub = pd.DataFrame()\n",
    "sub['sid'] = test_id\n",
    "sub['label']=prediction_lgb\n",
    "sub['label']=sub['label'].apply(lambda x: 1 if x>=0.5 else 0) ##∪概率大于0.5的置1，否则置0\n",
    "print('test pre_label distribution:\\n',sub['label'].value_counts()) ## 模型预测测试集的标签分布\n",
    "sub.to_csv('submit.csv',index=None) ##保存为submit0704.csv文件"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
