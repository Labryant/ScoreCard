{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入工具库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T13:18:24.256313Z",
     "start_time": "2020-01-05T13:18:13.348360Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T13:18:30.717059Z",
     "start_time": "2020-01-05T13:18:25.793188Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/data4/data/train.csv')\n",
    "entbase = pd.read_csv('../data/data4/data/1entbase.csv')\n",
    "alter = pd.read_csv('../data/data4/data/2alter.csv')\n",
    "branch = pd.read_csv('../data/data4/data/3branch.csv')\n",
    "invest = pd.read_csv('../data/data4/data/4invest.csv')\n",
    "right = pd.read_csv('../data/data4/data/5right.csv')\n",
    "project = pd.read_csv('../data/data4/data/6project.csv')\n",
    "lawsuit = pd.read_csv('../data/data4/data/7lawsuit.csv')\n",
    "breakfaith = pd.read_csv('../data/data4/data/8breakfaith.csv')\n",
    "recruit = pd.read_csv('../data/data4/data/9recruit.csv')\n",
    "qualification = pd.read_csv('../data/data4/data/10qualification.csv', encoding='gbk')\n",
    "test = pd.read_csv('../data/data4/data/evaluation_public.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理与特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_date(date):\n",
    "    year = int(date[:4])\n",
    "    month = int(date[-2:])\n",
    "    return (year - 2010) * 12 + month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得交叉特征，并以统计计数作为取值\n",
    "def get_interaction_feature(df, feature_A, feature_B):\n",
    "    feature_A_list = sorted(df[feature_A].unique())\n",
    "    feature_B_list = sorted(df[feature_B].unique())\n",
    "    count = 0\n",
    "    mydict = {}\n",
    "    for i in feature_A_list:\n",
    "        mydict[int(i)] = {}\n",
    "        for j in feature_B_list:\n",
    "            mydict[int(i)][int(j)] = count\n",
    "            count += 1\n",
    "    return df.apply(lambda x: mydict[int(x[feature_A])][int(x[feature_B])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entbase_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    mydf = df.fillna(value={'HY': 0, 'ZCZB': 0, 'MPNUM': 0, 'INUM': 0, 'ENUM': 0, 'FINZB': 0, 'FSTINUM': 0, 'TZINUM': 0})  # fill HY 为 0；ZCZB 为 0 表示缺失或错误\n",
    "\n",
    "    mydf['allnum'] = mydf['MPNUM'] + mydf['INUM'] + mydf['MPNUM'] + mydf['TZINUM'] + mydf['ENUM']\n",
    "\n",
    "    mydf['zczb/rgyear'] = mydf['ZCZB'] / mydf['RGYEAR']\n",
    "    mydf[\"rgyear_zczb\"] = get_interaction_feature(mydf, \"RGYEAR\", \"ZCZB\")\n",
    "    mydf['rgyear_finzb'] = get_interaction_feature(mydf, 'RGYEAR', 'FINZB')\n",
    "    mydf['rgyear_inum'] = get_interaction_feature(mydf, 'RGYEAR', 'INUM')\n",
    "    mydf['rgyear_enum'] = get_interaction_feature(mydf, 'RGYEAR', 'ENUM')\n",
    "    mydf['rgyear_fstinum'] = get_interaction_feature(mydf, 'RGYEAR', 'FSTINUM')\n",
    "    mydf['rgyear_tzinum'] = get_interaction_feature(mydf, 'RGYEAR', 'TZINUM')\n",
    "    mydf['rgyear_mpnum'] = get_interaction_feature(mydf, 'RGYEAR', 'MPNUM')\n",
    "    mydf['zczb_rgyear'] = get_interaction_feature(mydf, 'ZCZB', 'RGYEAR')\n",
    "    mydf['zczb_finzb'] = get_interaction_feature(mydf, 'ZCZB', 'FINZB')\n",
    "    mydf['zczb_inum'] = get_interaction_feature(mydf, 'ZCZB', 'INUM')\n",
    "    mydf['zczb_enum'] = get_interaction_feature(mydf, 'ZCZB', 'ENUM')\n",
    "    mydf['zczb_fstinum'] = get_interaction_feature(mydf, 'ZCZB', 'FSTINUM')\n",
    "    mydf['zczb_tzinum'] = get_interaction_feature(mydf, 'ZCZB', 'TZINUM')\n",
    "    mydf['zczb_mpnum'] = get_interaction_feature(mydf, 'ZCZB', 'MPNUM')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alter_feature(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    alt_no = df.groupby(['EID', 'ALTERNO']).size().reset_index()\n",
    "    alt_no = alt_no.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    alt_no.columns = ['EID', 'alt_count', 'alt_types_count']\n",
    "\n",
    "    alt_no_oh = df.groupby(['EID', 'ALTERNO']).size().unstack().reset_index()\n",
    "    alt_no_oh.columns = [i if i == 'EID' else 'alt_' + i for i in alt_no_oh.columns]\n",
    "\n",
    "    df['date'] = df['ALTDATE'].apply(translate_date)\n",
    "    date = df.groupby('EID')['date'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    date.columns = ['EID', 'alt_date_min', 'alt_date_max', 'alt_date_ptp', 'alt_date_std']\n",
    "\n",
    "    df['altbe'] = df['ALTBE'].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "    df['altaf'] = df['ALTAF'].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "    alt_be_af = df.groupby('EID')['altbe', 'altaf'].agg([min, max, np.mean]).reset_index()\n",
    "    alt_be_af.columns = ['EID', 'alt_be_min', 'alt_be_max', 'alt_be_mean', 'alt_af_min', 'alt_af_max', 'alt_af_mean']\n",
    "    \n",
    "\n",
    "    df['alt_be_af_gap'] = df['altaf'] - df['altbe']\n",
    "    alt_be_af_gap = df.groupby('EID')['alt_be_af_gap'].agg([min, max, np.mean, np.ptp, np.std]).reset_index()\n",
    "    alt_be_af_gap.columns = [i if i == 'EID' else 'alt_be_af_gap_' + i for i in alt_be_af_gap.columns]\n",
    "    \n",
    "    alt_1year = df[df['ALTDATE'] >= '2015-01'].groupby('EID').size().reset_index()\n",
    "    alt_1year.columns = ['EID', 'alt_num(1year)']\n",
    "    alt_2year = df[df['ALTDATE'] >= '2014-01'].groupby('EID').size().reset_index()\n",
    "    alt_2year.columns = ['EID', 'alt_num(2year)']\n",
    "    alt_3year = df[df['ALTDATE'] >= '2013-01'].groupby('EID').size().reset_index()\n",
    "    alt_3year.columns = ['EID', 'alt_num(3year)']\n",
    "\n",
    "    mydf = pd.merge(alt_no, alt_no_oh, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, alt_be_af, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, alt_be_af_gap, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, alt_1year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, alt_2year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, alt_3year, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_right_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    rig_type = df.groupby(['EID', 'RIGHTTYPE']).size().reset_index()\n",
    "    rig_type = rig_type.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rig_type.columns = ['EID', 'rig_count', 'rig_types_count']\n",
    "    \n",
    "    rig_type_oh_rate = df.groupby(['EID', 'RIGHTTYPE']).size().unstack().reset_index()\n",
    "    rig_type_oh_rate.iloc[:, 1:] = rig_type_oh_rate.iloc[:, 1:].div(rig_type['rig_count'], axis='index')\n",
    "    rig_type_oh_rate.columns = [i if i == 'EID' else 'rig_rate_' + str(i) for i in rig_type_oh_rate.columns]\n",
    "    \n",
    "    # 以下用法在版本更新后不能使用了，已做修改\n",
    "#     df['ask_month'] = (pd.to_datetime(df['ASKDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    df['ask_month'] = pd.to_datetime(df['ASKDATE']).apply(lambda x: 12*(x.year - 2010) + x.month - 1).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    ask_date = df.groupby('EID')['ask_month'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    ask_date.columns = ['EID', 'rig_askdate_max', 'rig_askdate_min', 'rig_askdate_ptp', 'rig_askdate_std']\n",
    "\n",
    "#     df['get_month'] = (pd.to_datetime(df['FBDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    df['get_month'] = pd.to_datetime(df['FBDATE']).apply(lambda x: 12*(x.year - 2010) + x.month - 1).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    get_date = df.groupby('EID')['get_month'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    get_date.columns = ['EID', 'rig_getdate_max', 'rig_getdate_min', 'rig_getdate_ptp', 'rig_getdate_std']\n",
    "    \n",
    "    unget = df[df.FBDATE.isnull()]\n",
    "    unget = unget.groupby('EID').size().reset_index()\n",
    "    unget.columns = ['EID', 'rig_unget_num']\n",
    "    \n",
    "    right_1year = df[df['ASKDATE'] >= '2015-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_1year.columns = ['EID', 'ask_num(1year)']\n",
    "    right_2year = df[df['ASKDATE'] >= '2014-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_2year.columns = ['EID', 'ask_num(2year)']\n",
    "    right_5year = df[df['ASKDATE'] >= '2010-01'].groupby('EID')['ASKDATE'].count().reset_index()\n",
    "    right_5year.columns = ['EID', 'ask_num(5year)']\n",
    "    right_end_1year = df[df['FBDATE'] >= '2015-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_1year.columns = ['EID', 'get_num(1year)']\n",
    "    right_end_2year = df[df['FBDATE'] >= '2014-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_2year.columns = ['EID', 'get_num(2year)']\n",
    "    right_end_5year = df[df['FBDATE'] >= '2010-01'].groupby('EID')['FBDATE'].count().reset_index()\n",
    "    right_end_5year.columns = ['EID', 'get_num(5year)']\n",
    "    \n",
    "    df['ask_get_month_gap'] = df['get_month'] - df['ask_month']\n",
    "    ask_get_month_gap = df.groupby('EID')['ask_get_month_gap'].agg([max, min, np.ptp, np.mean, np.std]).reset_index()\n",
    "    ask_get_month_gap.columns = [i if i == 'EID' else 'ask_get_month_gap_' + i for i in ask_get_month_gap.columns]\n",
    "    \n",
    "    mydf = pd.merge(rig_type, rig_type_oh_rate, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, ask_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, get_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, unget, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_1year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_2year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_5year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_1year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_2year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, right_end_5year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, ask_get_month_gap, how='left', on='EID')\n",
    "    \n",
    "    mydf['ask_rate(1year)'] = mydf['ask_num(1year)'] / mydf['rig_count']\n",
    "    mydf['ask_rate(2year)'] = mydf['ask_num(2year)'] / mydf['rig_count']\n",
    "    mydf['ask_rate(5year)'] = mydf['ask_num(5year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(1year)'] = mydf['get_num(1year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(2year)'] = mydf['get_num(2year)'] / mydf['rig_count']\n",
    "    mydf['get_rate(5year)'] = mydf['get_num(5year)'] / mydf['rig_count']\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recruit_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    rec_wz = df.groupby(['EID', 'WZCODE']).size().reset_index()\n",
    "    rec_wz = rec_wz.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rec_wz.columns = ['EID', 'rec_wz_count', 'rec_wz_types_count']\n",
    "    \n",
    "    rec_wz_oh = df.groupby(['EID', 'WZCODE']).size().unstack().reset_index()\n",
    "    rec_wz_oh.columns = [i if i == 'EID' else 'rec_wz_' + i for i in rec_wz_oh.columns]\n",
    "    \n",
    "    rec_pos = df.groupby(['EID', 'POSCODE']).size().reset_index()\n",
    "    rec_pos = rec_pos.groupby('EID')[0].agg([sum, len]).reset_index()\n",
    "    rec_pos.columns = ['EID', 'rec_pos_count', 'rec_pos_types_count']\n",
    "    \n",
    "#     df['recdate'] = (pd.to_datetime(df['RECDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    df['recdate'] = pd.to_datetime(df['RECDATE']).apply(lambda x: 12*(x.year - 2010) + x.month - 1).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    rec_date = df.groupby('EID')['recdate'].agg([max, min, np.ptp, np.std]).reset_index()\n",
    "    rec_date.columns = ['EID', 'rec_date_max', 'rec_date_min', 'rec_date_ptp', 'rec_date_std']\n",
    "    \n",
    "    df['pnum'] = df['PNUM'].str.extract('(\\d+)').fillna(1).astype(int)  # 若干=1\n",
    "    rec_num = df.groupby('EID')['pnum'].agg([sum, max, min, np.ptp, np.std]).reset_index()\n",
    "    rec_num.columns = ['EID' if i == 'EID' else 'rec_num_' + i for i in rec_num.columns]\n",
    "    \n",
    "    rec_count = df.groupby('EID').size().reset_index()\n",
    "    rec_count.columns = ['EID', 'rec_count']\n",
    "    \n",
    "    mydf = pd.merge(rec_wz, rec_wz_oh, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_pos, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_num, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, rec_count, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_branch_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    bra_num = df.groupby('EID')['TYPECODE'].size().reset_index()\n",
    "    bra_num.columns = ['EID', 'bra_count']\n",
    "    \n",
    "    bra_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    bra_home.columns = ['EID', 'bra_nothome', 'bra_home']\n",
    "    \n",
    "    bra_year = df.groupby('EID')['B_REYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bra_year.columns = [i if i == 'EID' else 'bra_year_' + i for i in bra_year.columns]\n",
    "    \n",
    "    bra_endyear = df.groupby('EID')['B_ENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bra_endyear.columns = [i if i == 'EID' else 'bra_endyear_' + i for i in bra_endyear.columns]\n",
    "    \n",
    "    bra_end_num = df[~df['B_ENDYEAR'].isnull()].groupby('EID').size().reset_index()\n",
    "    bra_end_num.columns = ['EID', 'bra_end_num']\n",
    "    bra_notend_num = df[df['B_ENDYEAR'].isnull()].groupby('EID').size().reset_index()\n",
    "    bra_notend_num.columns = ['EID', 'bra_notend_num']\n",
    "    \n",
    "    df['bra_begin_end_gap'] = df['B_ENDYEAR'] - df['B_REYEAR']\n",
    "    bra_begin_end_gap = df.groupby('EID')['bra_begin_end_gap'].agg([min, max, np.ptp, np.mean, np.std]).reset_index()\n",
    "    bra_begin_end_gap.columns = [i if i == 'EID' else 'bra_begin_end_gap_' + i for i in bra_begin_end_gap.columns]\n",
    "    \n",
    "    mydf = pd.merge(bra_num, bra_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_endyear, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_end_num, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_notend_num, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bra_begin_end_gap, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_invest_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    inv_num = df.groupby('EID').size().reset_index()\n",
    "    inv_num.columns = ['EID', 'inv_count']\n",
    "    \n",
    "    inv_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    inv_home.columns = ['EID', 'inv_nothome_num', 'inv_home_num']\n",
    "    \n",
    "    inv_bl = df.groupby('EID')['BTBL'].agg([sum, min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_bl.columns = [i if i == 'EID' else 'inv_bl_' + i for i in inv_bl.columns]\n",
    "    \n",
    "    inv_year = df.groupby('EID')['BTYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_year.columns = [i if i == 'EID' else 'inv_year_' + i for i in inv_year.columns]\n",
    "    \n",
    "    inv_endyear = df.groupby('EID')['BTENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inv_endyear.columns = [i if i == 'EID' else 'inv_endyear_' + i for i in inv_endyear.columns]\n",
    "    \n",
    "    inved_num = df.groupby('BTEID').size().reset_index()\n",
    "    inved_num.columns = ['EID', 'inved_num']\n",
    "    \n",
    "    inved_home = df.groupby(['BTEID', 'IFHOME']).size().unstack().reset_index()\n",
    "    inved_home.columns = ['EID', 'inved_nothome_num', 'inved_home_num']\n",
    "    \n",
    "    inved_bl = df.groupby('BTEID')['BTBL'].agg([sum, min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_bl.columns = ['EID' if i == 'BTEID' else 'inved_bl_' + i for i in inved_bl.columns]\n",
    "    \n",
    "    inved_year = df.groupby('BTEID')['BTYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_year.columns = ['EID' if i == 'BTEID' else 'inved_year_' + i for i in inved_year.columns]\n",
    "    \n",
    "    inved_endyear = df.groupby('BTEID')['BTENDYEAR'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    inved_endyear.columns = ['EID' if i == 'BTEID' else 'inved_endyear_' + i for i in inved_endyear.columns]\n",
    "    \n",
    "    df['inv_begin_end_gap'] = df['BTENDYEAR'] - df['BTYEAR']\n",
    "    inv_begin_end_gap = df.groupby('EID')['inv_begin_end_gap'].agg([min, max, np.ptp, np.mean, np.std]).reset_index()\n",
    "    inv_begin_end_gap.columns = [i if i == 'EID' else 'inv_begin_end_gap_' + i for i in inv_begin_end_gap.columns]\n",
    "    \n",
    "    mydf = pd.merge(inv_num, inv_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_bl, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_endyear, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_num, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_home, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_bl, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_year, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inved_endyear, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, inv_begin_end_gap, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lawsuit_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    law_num = df.groupby('EID').size().reset_index()\n",
    "    law_num.columns = ['EID', 'law_count']\n",
    "    \n",
    "    df['lawdate'] = df['LAWDATE'].apply(lambda x: x.replace('年', '-').replace('月', '')).apply(translate_date)\n",
    "    law_date = df.groupby('EID')['lawdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    law_date.columns = [i if i == 'EID' else 'law_date_' + i for i in law_date.columns]\n",
    "    \n",
    "    law_amout = df.groupby('EID')['LAWAMOUNT'].agg([sum, min, max, np.mean, np.ptp, np.std]).reset_index()\n",
    "    law_amout.columns = [i if i == 'EID' else 'law_amout_' + i for i in law_amout.columns]\n",
    "    \n",
    "    mydf = pd.merge(law_num, law_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, law_amout, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    pro_num = df.groupby('EID').size().reset_index()\n",
    "    pro_num.columns = ['EID', 'pro_count']\n",
    "    \n",
    "    df['djdate'] = df['DJDATE'].apply(translate_date)\n",
    "    pro_date = df.groupby('EID')['djdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    pro_date.columns = [i if i == 'EID' else 'pro_date_' + i for i in pro_date.columns]\n",
    "    \n",
    "    pro_home = df.groupby(['EID', 'IFHOME']).size().unstack().reset_index()\n",
    "    pro_home.columns = ['EID', 'pro_nothome_num', 'pro_home_num']\n",
    "    \n",
    "    mydf = pd.merge(pro_num, pro_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, pro_home, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qualification_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    qua_num = df.groupby('EID').size().reset_index()\n",
    "    qua_num.columns = ['EID', 'qua_count']\n",
    "    \n",
    "    qua_type = df.groupby(['EID', 'ADDTYPE']).size().unstack().reset_index()\n",
    "    qua_type.columns = [i if i == 'EID' else 'qua_type_' + str(i) for i in qua_type.columns]\n",
    "    \n",
    "    df['begindate'] = df['BEGINDATE'].apply(lambda x: x.replace(u'年', '-').replace(u'月', '')).apply(translate_date)\n",
    "    qua_begindate = df.groupby('EID')['begindate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    qua_begindate.columns = [i if i == 'EID' else 'qua_begindate_' + i for i in qua_begindate.columns]\n",
    "    \n",
    "    df['expirydate'] = df['EXPIRYDATE'].apply(lambda x: x.replace(u'年', '-').replace(u'月', '') if not pd.isnull(x) else np.nan)\n",
    "#     df['expirydate'] = (pd.to_datetime(df['expirydate']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    df['expirydate'] = pd.to_datetime(df['expirydate']).apply(lambda x: 12*(x.year - 2010) + x.month - 1).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    qua_expirydate = df.groupby('EID')['expirydate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    qua_expirydate.columns = [i if i == 'EID' else 'qua_expirydate_' + i for i in qua_expirydate.columns]\n",
    "    \n",
    "    df['qua_begin_expiry_gap'] = df['expirydate'] - df['begindate']\n",
    "    qua_begin_expiry_gap = df.groupby('EID')['qua_begin_expiry_gap'].agg([min, max, np.ptp, np.mean, np.std]).reset_index()\n",
    "    qua_begin_expiry_gap.columns = [i if i == 'EID' else 'qua_begin_expiry_gap_' + i for i in qua_begin_expiry_gap.columns]\n",
    "    \n",
    "    mydf = pd.merge(qua_num, qua_type, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, qua_begindate, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, qua_expirydate, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, qua_begin_expiry_gap, how='left', on='EID')\n",
    "    \n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breakfaith_feature(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    bre_num = df.groupby('EID').size().reset_index()\n",
    "    bre_num.columns = ['EID', 'bre_count']\n",
    "    \n",
    "    df['fbdate'] = df['FBDATE'].apply(lambda x: x.replace('年', '-').replace('月', '')).apply(translate_date)\n",
    "    bre_date = df.groupby('EID')['fbdate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bre_date.columns = [i if i == 'EID' else 'bre_date_' + i for i in bre_date.columns]\n",
    "    \n",
    "#     df['sxenddate'] = (pd.to_datetime(df['SXENDDATE']).dt.to_period(\"M\") - (pd.to_datetime('2010-01-01').to_period(\"M\"))).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    df['sxenddate'] = pd.to_datetime(df['SXENDDATE']).apply(lambda x: 12*(x.year - 2010) + x.month - 1).fillna(-999).astype(int).replace(-999, np.NaN)\n",
    "    bre_enddate = df.groupby('EID')['sxenddate'].agg([min, max, np.ptp, np.std]).reset_index()\n",
    "    bre_enddate.columns = [i if i == 'EID' else 'bre_enddate_' + i for i in bre_enddate.columns]\n",
    "    \n",
    "    df['bre_begin_end_gap'] = df['sxenddate'] - df['fbdate']\n",
    "    bre_begin_end_gap = df.groupby('EID')['bre_begin_end_gap'].agg([min, max, np.ptp, np.mean, np.std]).reset_index()\n",
    "    bre_begin_end_gap.columns = [i if i == 'EID' else 'bre_begin_end_gap_' + i for i in bre_begin_end_gap.columns]\n",
    "    \n",
    "    mydf = pd.merge(bre_num, bre_date, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bre_enddate, how='left', on='EID')\n",
    "    mydf = pd.merge(mydf, bre_begin_end_gap, how='left', on='EID')\n",
    "\n",
    "    return mydf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entbase_feat = get_entbase_feature(entbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter_feat = get_alter_feature(alter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_feature = get_right_feature(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recruit_feat = get_recruit_feature(recruit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_feat = get_branch_feature(branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invest_feat = get_invest_feature(invest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lawsuit_feat = get_lawsuit_feature(lawsuit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_feat = get_project_feature(project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualification_feat = get_qualification_feature(qualification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakfaith_feat = get_breakfaith_feature(breakfaith)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(entbase_feat, alter_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, right_feature, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, recruit_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, branch_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, invest_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, lawsuit_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, project_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, qualification_feat, on='EID', how='left')\n",
    "dataset = pd.merge(dataset, breakfaith_feat, on='EID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['alt_count/rgyear'] = dataset['alt_count'] / dataset['RGYEAR']\n",
    "dataset['rig_count/rgyear'] = dataset['rig_count'] / dataset['RGYEAR']\n",
    "dataset['rec_count/rgyear'] = dataset['rec_count'] / dataset['RGYEAR']\n",
    "dataset['bra_count/rgyear'] = dataset['bra_count'] / dataset['RGYEAR']\n",
    "dataset['inv_count/rgyear'] = dataset['inv_count'] / dataset['RGYEAR']\n",
    "dataset['inved_num/rgyear'] = dataset['inved_num'] / dataset['RGYEAR']\n",
    "dataset['law_count/rgyear'] = dataset['law_count'] / dataset['RGYEAR']\n",
    "dataset['pro_count/rgyear'] = dataset['pro_count'] / dataset['RGYEAR']\n",
    "dataset['qua_count/rgyear'] = dataset['qua_count'] / dataset['RGYEAR']\n",
    "dataset['bre_count/rgyear'] = dataset['bre_count'] / dataset['RGYEAR']\n",
    "\n",
    "dataset['alt_num(1year)/rgyear'] = dataset['alt_num(1year)'] / dataset['RGYEAR']\n",
    "dataset['alt_num(2year)/rgyear'] = dataset['alt_num(2year)'] / dataset['RGYEAR']\n",
    "dataset['alt_num(3year)/rgyear'] = dataset['alt_num(3year)'] / dataset['RGYEAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['MPNUM_CLASS'] = dataset['INUM'].apply(lambda x : x if x <= 4 else 5)\n",
    "dataset['FSTINUM_CLASS'] = dataset['FSTINUM'].apply(lambda x : x if x <= 6 else 7)\n",
    "dataset.fillna(value={'alt_count': 0, 'rig_count': 0}, inplace=True)\n",
    "for column in ['MPNUM', 'INUM', 'FINZB', 'FSTINUM', 'TZINUM', 'ENUM', 'ZCZB', 'allnum', 'RGYEAR', 'alt_count', 'rig_count']:\n",
    "    groupby_list = [['HY'], ['ETYPE'], ['HY', 'ETYPE'], ['HY', 'PROV'], ['ETYPE', 'PROV'], ['MPNUM_CLASS'], ['FSTINUM_CLASS']]\n",
    "    for groupby in groupby_list:\n",
    "        if 'MPNUM_CLASS' in groupby and column == 'MPNUM':\n",
    "            continue\n",
    "        if 'FSTINUM_CLASS' in groupby and column == 'FSTINUM':\n",
    "            continue\n",
    "        groupby_keylist = []\n",
    "        for key in groupby:\n",
    "            groupby_keylist.append(dataset[key])\n",
    "        tmp = dataset[column].groupby(groupby_keylist).agg([sum, min, max, np.mean]).reset_index()\n",
    "        tmp = pd.merge(dataset, tmp, on=groupby, how='left')\n",
    "        dataset['ent_' + column.lower() + '-mean_gb_' + '_'.join(groupby).lower()] = dataset[column] - tmp['mean']\n",
    "        dataset['ent_' + column.lower() + '-min_gb_' + '_'.join(groupby).lower()] = dataset[column] - tmp['min']\n",
    "        dataset['ent_' + column.lower() + '-max_gb_' + '_'.join(groupby).lower()] = dataset[column] - tmp['max']\n",
    "        dataset['ent_' + column.lower() + '/sum_gb_' + '_'.join(groupby).lower()] = dataset[column] / tmp['sum']\n",
    "dataset.drop(['MPNUM_CLASS', 'FSTINUM_CLASS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.join(pd.get_dummies(dataset['PROV'], prefix='prov'))\n",
    "dataset = dataset.join(pd.get_dummies(dataset['HY'], prefix='hy').mul(dataset['ZCZB'], axis=0))\n",
    "dataset = dataset.join(pd.get_dummies(dataset['ETYPE'], prefix='etype').mul(dataset['RGYEAR'], axis=0))\n",
    "dataset.drop(['PROV', 'HY', 'ETYPE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = pd.merge(train, dataset, on='EID', how='left')\n",
    "testset = pd.merge(test, dataset, on='EID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = trainset.drop(['TARGET', 'ENDDATE'], axis=1)\n",
    "train_label = trainset.TARGET.values\n",
    "test_feature = testset\n",
    "test_index = testset.EID.values\n",
    "print train_feature.shape, train_label.shape, test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EID 前面的字母代表不同省份，已提供了 PROV 列，因此字母是冗余信息，直接舍弃\n",
    "train_feature['EID'] = train_feature['EID'].str.extract('(\\d+)').astype(int)\n",
    "test_feature['EID'] = test_feature['EID'].str.extract('(\\d+)').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建模与调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'rounds': 100,    # 这里为了跑的更快，把参数做了修改，如果这里大家不想等可以设置的再小点\n",
    "    'folds': 3\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic',\n",
    "    'stratified': True,\n",
    "    'scale_pos_weights ': 0,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 15,\n",
    "#     'gamma': 0.1,\n",
    "    'subsample': 0.75,\n",
    "    'colsample_bytree': 0.75,\n",
    "#     'lambda': 1,\n",
    "\n",
    "    'eta': 0.01,\n",
    "    'seed': 42,\n",
    "    'silent': 1,\n",
    "    'eval_metric': 'auc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_cv(train_feature, train_label, params, folds, rounds):\n",
    "    start = time.clock()\n",
    "    print train_feature.columns\n",
    "    params['scale_pos_weights '] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    num_round = rounds\n",
    "    print 'run cv: ' + 'round: ' + str(rounds)\n",
    "    res = xgb.cv(params, dtrain, num_round, nfold=folds, verbose_eval=10, early_stopping_rounds=100)\n",
    "    elapsed = (time.clock() - start)\n",
    "    print 'Time used:', elapsed, 's'\n",
    "    return len(res), res.loc[len(res) - 1, 'test-auc-mean']\n",
    "\n",
    "\n",
    "def xgb_predict(train_feature, train_label, test_feature, rounds, params):\n",
    "    params['scale_pos_weights '] = float(len(train_label[train_label == 0])) / len(train_label[train_label == 1])\n",
    "    dtrain = xgb.DMatrix(train_feature, label=train_label)\n",
    "    dtest = xgb.DMatrix(test_feature, label=np.zeros(test_feature.shape[0]))\n",
    "    watchlist = [(dtrain, 'train')]\n",
    "    num_round = rounds\n",
    "    model = xgb.train(params, dtrain, num_round, watchlist, verbose_eval=30)\n",
    "    predict = model.predict(dtest)\n",
    "    return model, predict\n",
    "\n",
    "\n",
    "def store_result(test_index, pred, threshold, name):\n",
    "    result = pd.DataFrame({'EID': test_index, 'FORTARGET': 0, 'PROB': pred})\n",
    "    mask = result['PROB'] >= threshold\n",
    "    result.at[mask, 'FORTARGET'] = 1\n",
    "    result.to_csv('../../tmp/' + name + '.csv', index=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterations, best_score = xgb_cv(train_feature, train_label, params, config['folds'], config['rounds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个库没有什么用，就是发出声音的，服务器上无法执行，大家可以在本地试试\n",
    "# 其中600表示声音大小，1000表示发生时长，1000为1秒\n",
    "# import winsound\n",
    "# winsound.Beep(600,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, pred = xgb_predict(train_feature, train_label, test_feature, iterations, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame(model.get_fscore().items(), columns=['feature','importance']).sort_values('importance', ascending=False)\n",
    "importance.to_csv('../../tmp/importance-1207-%f(r%d).csv' % (best_score, iterations), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = store_result(test_index, pred, 0.18, '1207-xgb-%f(r%d)' % (best_score, iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cv(train_feat, labels):\n",
    "#     params={       \n",
    "#         'booster': 'gbtree',\n",
    "#         'objective': 'binary:logistic',\n",
    "#         'stratified': True,\n",
    "#         'scale_pos_weights ': 0,\n",
    "#         'max_depth': 8,\n",
    "#         'min_child_weight': 1,\n",
    "# #         'gamma': 0.1,\n",
    "#         'subsample': 0.75,\n",
    "# #         'colsample_bytree': 0.75,\n",
    "# #         'lambda': 1,\n",
    "\n",
    "#         'eta': 0.02,\n",
    "#         'seed': 42,\n",
    "#         'silent': 1,\n",
    "#         'eval_metric': 'auc'\n",
    "#     }\n",
    "#     res = xgb.cv(params, xgb.DMatrix(train_feat, label=labels), 2000, nfold=3, early_stopping_rounds=50, verbose_eval=300)\n",
    "#     return res.tail(1)[\"test-auc-mean\"].values[0]\n",
    "\n",
    "# def auto_feature(train_feat, labels):\n",
    "#     import copy\n",
    "#     columns = train_feat.columns\n",
    "#     old_feature_list = []\n",
    "#     curr_feature_list = []\n",
    "#     max_score = 0.5\n",
    "#     good_feature = []\n",
    "#     bad_feature = []\n",
    "\n",
    "#     for feature in columns:\n",
    "#         old_feature_list = copy.deepcopy(curr_feature_list)\n",
    "#         curr_feature_list.append(feature)\n",
    "\n",
    "#         score = cv(train_feat[curr_feature_list], labels)\n",
    "\n",
    "#         if score > max_score - 0.0003:\n",
    "#             good_file = open(\"../data/output/feat/good.txt\", \"a\")\n",
    "#             gap = score - max_score\n",
    "#             output_str = \"\"\n",
    "#             for i in old_feature_list:\n",
    "#                 output_str += i + \",\"\n",
    "#             output_str += \"\\nold score:\" + str(max_score) +\"\\n\"\n",
    "#             output_str += \"Test feature: \" + feature + \" +\" + str(gap) + \", now: \" + str(score) + \"\\n\"\n",
    "#             output_str += \"--------------------------------------------------------\\n\"\n",
    "#             max_score = score\n",
    "#             good_file.write(output_str)\n",
    "#             good_file.close()\n",
    "#             print output_str\n",
    "#         else:\n",
    "#             bad_file = open(\"../data/output/feat/bad.txt\", \"a\")\n",
    "#             curr_feature_list = copy.deepcopy(old_feature_list) #recover\n",
    "#             gap = max_score - score\n",
    "#             output_str = \"\"\n",
    "#             for i in old_feature_list:\n",
    "#                 output_str += i + \",\"\n",
    "#             output_str += \"\\n old score:\" + str(max_score) +\"\\n\"\n",
    "#             output_str += \"Test feature:\" + feature + \" -\" + str(gap) + \", now: \" + str(score) + \"\\n\"\n",
    "#             output_str += \"--------------------------------------------------------\\n\"\n",
    "\n",
    "#             bad_file.write(output_str)\n",
    "#             bad_file.close()\n",
    "#             print output_str\n",
    "\n",
    "# auto_feature(train_feature, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LogisticRegression\n",
    "# lr_params = {\n",
    "#     'C': 2\n",
    "# }\n",
    "# # RandomForest\n",
    "# rf_params = {\n",
    "#     'n_jobs': -1,\n",
    "#     'n_estimators': 1800,\n",
    "#     'max_features': 0.8,\n",
    "#     'max_depth': 15,\n",
    "#     'min_samples_leaf': 5\n",
    "# }\n",
    "# # ExtraTrees \n",
    "# et_params = {\n",
    "#     'n_jobs': -1,\n",
    "#     'n_estimators': 2000,\n",
    "#     'max_features': 0.8,\n",
    "#     'max_depth': 20,\n",
    "#     'min_samples_leaf': 5\n",
    "# }\n",
    "# # BGDT\n",
    "# gb_params = {\n",
    "#     'n_estimators': 1200,\n",
    "#     'subsample':0.8,\n",
    "#     'max_features': 0.8,                                                                                                                                                                                                                                                                                                                  \n",
    "#     'max_depth': 10,\n",
    "#     'min_samples_leaf': 3,\n",
    "#     'learning_rate':0.02,\n",
    "#     'verbose': 0\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################ lr stack ############################\n",
    "# seed = 2017\n",
    "# num_class = 2\n",
    "# n = 5\n",
    "\n",
    "# x = train_feature.fillna(0)\n",
    "# y = train_label\n",
    "# x_te = test_feature.fillna(0)\n",
    "\n",
    "# stack = np.zeros((x.shape[0], num_class))\n",
    "# stack_te = np.zeros((x_te.shape[0], num_class))\n",
    "\n",
    "# for i, (tr, va) in enumerate(StratifiedKFold(y, n_folds=n, random_state=seed)):\n",
    "#     print('stack:%d/%d' % ((i + 1), n))\n",
    "#     clf = LogisticRegression(**lr_params)\n",
    "#     clf.fit(x.iloc[tr], y[tr])\n",
    "#     y_pred_va = clf.predict_proba(x.iloc[va])\n",
    "#     y_pred_te = clf.predict_proba(x_te)\n",
    "#     stack[va] += y_pred_va\n",
    "#     stack_te += y_pred_te\n",
    "# stack_te /= n\n",
    "\n",
    "# df_stack = pd.DataFrame(stack)\n",
    "# df_stack.columns = ['lr_' + str(i) for i in df_stack.columns]\n",
    "# df_stack_te = pd.DataFrame(stack_te)\n",
    "# df_stack_te.columns = ['lr_' + str(i) for i in df_stack_te.columns]\n",
    "# df_stack.to_csv('../data/output/feat/stack/lr_prob.csv', index=None, encoding='utf8')\n",
    "# df_stack_te.to_csv('../data/output/feat/stack/lr_prob_te.csv', index=None, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################ ExtraTreesClassifier stack ############################\n",
    "# seed = 2017\n",
    "# num_class = 2\n",
    "# n = 3\n",
    "\n",
    "# x = train_feature.fillna(0)\n",
    "# y = train_label\n",
    "# x_te = test_feature.fillna(0)\n",
    "\n",
    "# stack = np.zeros((x.shape[0], num_class))\n",
    "# stack_te = np.zeros((x_te.shape[0], num_class))\n",
    "\n",
    "# for i, (tr, va) in enumerate(StratifiedKFold(y, n_folds=n, random_state=seed)):\n",
    "#     print('stack:%d/%d' % ((i + 1), n))\n",
    "#     clf = ExtraTreesClassifier(**et_params)\n",
    "#     clf.fit(x.iloc[tr], y[tr])\n",
    "#     y_pred_va = clf.predict_proba(x.iloc[va])\n",
    "#     y_pred_te = clf.predict_proba(x_te)\n",
    "#     stack[va] += y_pred_va\n",
    "#     stack_te += y_pred_te\n",
    "# stack_te /= n\n",
    "\n",
    "# df_stack = pd.DataFrame(stack)\n",
    "# df_stack.columns = ['et_' + str(i) for i in df_stack.columns]\n",
    "# df_stack_te = pd.DataFrame(stack_te)\n",
    "# df_stack_te.columns = ['et_' + str(i) for i in df_stack_te.columns]\n",
    "# df_stack.to_csv('../data/output/feat/stack/et_prob.csv', index=None, encoding='utf8')\n",
    "# df_stack_te.to_csv('../data/output/feat/stack/et_prob_te.csv', index=None, encoding='utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
